\chapter{Implementation}
\label{sec:implementation}

This chapter details the complete software implementation of the GPU-accelerated Cuckoo filter, translating the theoretical concepts from the previous chapter into a concrete, high-performance software artifact. The primary goal of the implementation is to provide a library that is not only fast but also robust, configurable, and scalable.

The chapter begins by outlining the fundamental design of the data structure itself, covering its compile-time configuration options via a flexible template-based system, its memory layout optimized for parallel hardware, and its comprehensive public-facing API. Following this, the chapter dives into the core of the implementation: the parallel algorithms for insertion, lookup, and deletion. Each operation is presented as a CUDA kernel, with detailed explanations of the logic used to handle concurrency, manage the eviction process, and ensure correctness.

Beyond the baseline algorithms, some advanced optimization techniques are explored. These include a sorted insertion strategy to improve memory locality for large, out-of-cache filters, and an alternative eviction method designed to reduce the amount of necessary evictions at high load factors.

Finally, two major extensions are presented that elevate the filter from a single-GPU data structure to a system-level component. An Inter-Process Communication (IPC) wrapper is detailed, which enables zero-copy sharing of the filter between multiple processes on the same machine. To address datasets larger than a single GPU's memory, the following section then describes a multi-GPU implementation that partitions the workload and data across multiple devices, using high-speed interconnects for efficient communication.

\section{Challenges}
\label{sec:implementation:challenges}

Based on the GPU architecture and programming principles discussed in Chapter \ref{sec:background}, translating the Cuckoo filter algorithm into a performant parallel implementation presents several specific technical hurdles. The following challenges directly influenced the design and optimization of the kernels detailed in this chapter:

\begin{itemize}
  \item \textbf{Managing Concurrency and Race Conditions}: A naive parallel implementation where thousands of GPU threads attempt to read and write to the same buckets simultaneously would lead to race conditions and data corruption. Designing an efficient, lock-free mechanism using atomic operations to handle these concurrent memory accesses is one of the primary challenges.

  \item \textbf{Parallelizing the Eviction Path}: During an insertion, if both candidate buckets are full, an existing item must be evicted and reinserted into its alternate location. This eviction process can cascade, leading to multiple displacements. Parallelizing this inherently path-dependent process is complex, as it requires careful coordination to ensure that threads do not interfere with each other's operations or create deadlocks.

  \item \textbf{Optimizing for Coalesced Memory Access}: GPUs are highly sensitive to memory access patterns. Coalesced memory accesses, where threads in a warp access contiguous memory locations, are crucial for achieving high throughput. The random-access nature of Cuckoo filter operations can lead to scattered, uncoalesced memory accesses, which severely degrade performance. Developing strategies like data sorting to improve memory locality is essential.

  \item \textbf{Balancing Occupancy and Register Pressure}: Achieving a high and balanced occupancy across the filter is key to its space efficiency. However, complex insertion logic can increase the number of registers used per thread. High register usage can limit the number of active warps on a Streaming Multiprocessor (SM), thereby reducing occupancy and the hardware's ability to hide memory latency.
\end{itemize}

\section{Data Structure Design}
\label{sec:implementation:data-structure-design}

The design of the GPU-accelerated Cuckoo filter emphasizes compile-time configuration and a memory layout optimized for parallel access patterns. This section details these design choices, from the high-level configuration structure down to the public-facing API.

\subsection{Compile-Time Configuration}
\label{sec:implementation:data-structure-design:comptime-config}

To maximize performance by allowing the compiler to generate highly specialized code, the filter's core parameters are defined at compile-time as template arguments. These parameters are consolidated into a single configuration structure, \texttt{CuckooConfig}, which provides a clean and explicit way to instantiate the filter. The primary configuration options are:

\begin{itemize}
  \item \texttt{bitsPerTag}: Defines the size of each fingerprint in bits. The implementation is specialized for values of 8, 16, or 32, which align well with standard integer types.

  \item \texttt{bucketSize}: Specifies the number of fingerprints stored in each bucket. A smaller bucket size generally leads to a lower false positive rate but can negatively impact insertion performance and overall occupancy. A default of 16 was chosen through empirical performance testing.

  \item \texttt{maxEvictions}: Sets the maximum number of displacements a single thread is allowed to perform during an insertion attempt before it gives up and reports a failure. The default value of 500 was determined empirically.

  \item \texttt{blockSize}: Configures the thread block size for the internal CUDA kernels. This parameter can be tuned to optimize GPU occupancy for different hardware architectures.

  \item \texttt{HashStrategy}: A template parameter that accepts a strategy class. This class encapsulates the logic for hashing items, calculating the required number of buckets, and deriving an item's alternate bucket from its primary one. The default implementation uses the standard XOR-based partial-key cuckoo hashing, but an alternative "Additive and Subtractive Cuckoo Filter" strategy has also been implemented to demonstrate this flexibility.
\end{itemize}

Throughout the implementation, \texttt{static\_assert} statements are used to enforce critical invariants at compile time, such as ensuring that certain parameters are powers of two to allow for efficient bitwise AND operations instead of more costly modulo arithmetic.

\subsection{Memory Layout and State Management}
\label{sec:implementation:mem-layout-state}

The filter's primary data storage is a single, contiguous linear array of fixed-size buckets allocated in the GPU's global memory. To maximize memory bandwidth and avoid misaligned access, the internal layout is carefully structured:

\begin{itemize}
  \item Each bucket is composed of an array of 64-bit unsigned integer words.

  \item Fingerprints (\texttt{tags}) are tightly packed within these 64-bit words. For example, a 64-bit word can hold eight 8-bit fingerprints or four 16-bit fingerprints.
\end{itemize}

This packed layout necessitates the use of bitwise shift and mask operations to extract individual fingerprints. However, the computational cost of these operations is negligible compared to the latency of memory access, making this a highly beneficial trade-off.

The filter's state is maintained by a single global atomic counter that tracks the total number of occupied slots. This counter resides in device memory and is updated atomically by the insertion and deletion kernels. Its value is only lazily copied to the host when an explicit query is made, minimizing host-device communication.

Due to a limitation of CUDA C++ where kernels cannot be class members, the implementation passes a pointer to the filter's class instance (\texttt{this}) to each launched kernel. This allows the threads to access the filter's configuration and state, such as data pointers and capacity, as needed.

\subsection{Public API}
\label{sec:implementation:api}

The filter exposes a comprehensive public interface for easy integration. Two sets of APIs are provided: a "traditional" C-style API that operates on raw device pointers and item counts, and, if the Thrust library is available, a set of convenience wrappers that accept \texttt{thrust::device\_vector} objects.

\subsubsection{\texttt{explicit CuckooFilter(size\_t capacity)}}

The constructor takes a single argument for the desired capacity in terms of the number of items. It guarantees that at least this many items can be stored. Depending on the chosen \texttt{HashStrategy}, the actual allocated capacity may be larger. For example, the default XOR-based strategy requires the number of buckets to be a power of two, so the requested capacity is rounded up to the next suitable size.

\subsubsection{\texttt{size\_t insertMany(const T* d\_keys, const size\_t n)}}

This function attempts to insert a batch of \texttt{n} items from a buffer in device memory pointed to by \texttt{d\_keys}. It is the caller's responsibility to ensure the pointer is valid and the buffer is sufficiently large. The function returns the updated total number of occupied slots in the filter after the insertion attempt.

\subsubsection{\texttt{size\_t insertManySorted(const T* d\_keys, const size\_t n)}}

This variant offers an alternative insertion strategy focused on improving memory locality. Before insertion, it first computes each item's primary bucket index and packs it with its fingerprint. This array of packed tags is then sorted on the GPU using CUB's radix sort. This ensures that threads with adjacent indices are highly likely to be accessing the same memory region. This approach generally outperforms the standard \texttt{insertMany} when the filter is too large to fit into the GPU's L2 cache. However, it incurs higher peak memory usage and is slower for smaller, cache-resident filters due to the overhead of the packing and sorting steps.

\subsubsection{\texttt{void containsMany(const T* d\_keys, const size\_t n, bool* d\_output)}}

Performs a batch lookup for \texttt{n} items. The results are written to the \texttt{d\_output} device buffer, where the boolean at each index corresponds to whether the item at the same index in \texttt{d\_keys} was found.

\subsubsection{\texttt{size\_t deleteMany(const T* d\_keys, const size\_t n, bool* d\_output = nullptr)}}

Performs a batch deletion of \texttt{n} items. If the optional \texttt{d\_output} buffer is provided, the success or failure of each individual deletion is reported in the same manner as the lookup operation.

\subsubsection{\texttt{void clear()}}

Resets the internal state of the filter by setting all buckets and the occupancy counter to zero. This operation does not deallocate any device memory.

\subsubsection{\texttt{float loadFactor()}}

Returns the current load factor of the filter, calculated as the number of occupied slots divided by the total capacity.

\subsubsection{\texttt{size\_t countOccupiedSlots()}}

A debugging utility that provides a ground-truth count of occupied slots. It operates by copying the entire filter data to the host and manually counting every non-zero fingerprint. As this is an extremely slow, synchronous operation, it should only be used for verification purposes if the internal atomic counter is suspected to be inaccurate.

\section{Parallel Algorithms}
\label{sec:implementation:parallel-algorithms}

The core of this thesis is the design of parallel algorithms for the Cuckoo filter's primary operations: insertion, lookup, and deletion. These algorithms are designed to be launched as CUDA kernels, where many threads cooperate to process batches of items simultaneously.

\subsection{Insertion}
\label{sec:implementation:insertion}

The parallel insertion algorithm is designed to handle a large batch of items in parallel, with each CUDA thread being responsible for inserting a single item. The process for each thread is as follows:

\begin{enumerate}
  \item \textbf{Hashing and Key Generation}: Each item is first hashed into a 64-bit value using the xxHash64 algorithm, chosen for its high speed and excellent statistical properties. This hash is then split: the upper 32 bits serve are used to derive the item's fingerprint, while the lower 32 bits are used as the basis for its primary bucket index. A crucial initial finding was that using the same bits for both the fingerprint and the primary index led to a high probability of identical fingerprints clustering in the same buckets, causing a severe degradation in performance. Thus, distinct parts of the hash are used for each. The alternate bucket index is then calculated using the partial-key cuckoo hashing scheme, which is described in detail in Section \ref{sec:background:cuckoo-filter}.

  \item \textbf{Direct Insertion Attempt}: The thread first checks the two candidate buckets for an empty slot. The thread uses the item's fingerprint to generate a random starting offset within each bucket and performs a linear search from that point. If an empty slot is found in either bucket, the fingerprint is inserted via an atomic operation, and the insertion for that item succeeds.

  \item \textbf{Eviction Process}: If both candidate buckets are full, the thread initiates the cuckoo process. It randomly selects one of the two buckets and a random occupied slot within it. It then atomically replaces the fingerprint (\texttt{tag\_old}) in that slot with its own fingerprint (\texttt{tag\_new}). The evicted fingerprint, \texttt{tag\_old}, now becomes the item that the thread must insert. The thread calculates \texttt{tag\_old}'s alternate bucket and continues this process in a loop.

  \item \textbf{Termination}: The eviction loop continues until either an empty slot is found or a predefined limit on the number of displacements is reached, at which point the insertion is reported as a failure.

\end{enumerate}

To maintain an accurate count of the total items in the filter without creating a bottleneck on a single atomic counter, a hierarchical reduction is employed. Each thread that successfully inserts an item contributes a +1. These values are first summed efficiently at the warp level using shuffle instructions, then aggregated at the block level using shared memory, and finally, a single atomic addition per block is performed on the global counter in device memory.

% FIXME: I don't really like this formatting but that's okay, can always change it later
% tex-fmt: off
\begin{algorithm}[H]
  \caption{Parallel Insertion}
  \label{alg:parallel-insertion}
\begin{algorithmic}[0]
  \Function{Insert}{key}
    \State $h = \mathrm{hash}(\text{key})$
    \State $fp = \mathrm{upper32}(h) \mathbin{\&} \bigl((1 \ll \text{bitsPerTag}) - 1\bigr)$
    \State $i_1 = \mathrm{lower32}(h) \bmod \text{numBuckets}$
    \State $i_2 = i_1 \oplus \mathrm{hash}(fp) \bmod \text{numBuckets}$
    \If{there is an empty slot in bucket $i_1$ or $i_2$}
      \State insert $fp$ into an empty slot (atomic)
      \State \textbf{return} \texttt{Success}
    \Else
      \State $\text{tag} = fp$
      \State $b =$ randomly pick $i_1$ or $i_2$
      \For{$n = 1$ \textbf{to} \text{maxEvictions}}
        \State $s =$ random slot in bucket $b$
        \State $\text{tag} = \text{atomic\_exchange}(buckets[b][s],\ \text{tag})$
        \State $b = \text{alt\_bucket}(b, \text{tag})$
        \If{there is an empty slot in bucket $b$}
          \State insert \text{tag} into an empty slot (atomic)
          \State \textbf{return} \texttt{Success}
        \EndIf
      \EndFor
      \LComment{Filter is considered full}
      \LComment{Caller will have to increase its capacity}
      \State \textbf{return} \texttt{Failure}
    \EndIf
  \EndFunction
\end{algorithmic}
\end{algorithm}
% tex-fmt: on

\subsection{Lookup}
\label{sec:implementation:lookup}

The parallel lookup algorithm is similar to insertion but is a read-only operation, allowing for more aggressive memory access optimizations. Each thread is assigned an item to look for.

The thread calculates the item's fingerprint and its two candidate bucket indices, just as in the insertion process. It then searches both buckets for a matching fingerprint, again using the fingerprint to determine a random starting point for a linear probe.

The key optimization in the lookup kernel is the use of vectorized, non-atomic memory loads. Since it can be guaranteed that no other kernel is writing to the filter during a lookup operation, thread safety is not a concern. Modern GPU hardware can load 128 bits (or 16 bytes) in a single instruction. The kernel leverages this by loading two 64-bit words at a time, allowing a thread to check multiple fingerprints with a single memory transaction and dramatically increasing throughput. The results of the lookup (found or not found) are written to a user-provided output array.

% tex-fmt: off
\begin{algorithm}[H]
  \caption{Parallel Lookup}
  \label{alg:parallel-lookup}
\begin{algorithmic}[0]
  \Function{Contains}{key}
    \State $h = \mathrm{hash}(\text{key})$
    \State $fp = \mathrm{upper32}(h) \mathbin{\&} \bigl((1 \ll \text{bitsPerTag}) - 1\bigr)$
    \State $i_1 = \mathrm{lower32}(h) \bmod \text{numBuckets}$
    \State $i_2 = i_1 \oplus \mathrm{hash}(fp) \bmod \text{numBuckets}$
    \State \textbf{return} FindSlot(bucket $i_1$, $fp$) \textbf{or} FindSlot(bucket $i_2$, $fp$)
  \EndFunction
  \Statex
  \Function{FindSlot}{bucket, tag}
    \State startSlot $=$ tag $\bmod$ bucketSize
    \State startWord $=$ startSlot / tagsPerWord
    \State startPair $=$ floorToEven(startWord)

    \For{$i = 0$ \textbf{to} wordCount / 2}
        \State pairIdx $=$ (startPair + i * 2) $\bmod$ wordCount
        \State word1, word2 $=$ loadTwoWordsAt(bucket, pairIdx)

        \For{each word \textbf{in} \{word1, word2\}}
            \For{$j = 0$ \textbf{to} tagsPerWord}
                \If{extractTag(word, $j$) == tag}
                    \State \textbf{return} \texttt{Success}
                \EndIf
            \EndFor
        \EndFor
    \EndFor
    \State \textbf{return} \texttt{Failure}
  \EndFunction
\end{algorithmic}
\end{algorithm}
% tex-fmt: on
\subsection{Deletion}
\label{sec:implementation:deletion}

The parallel deletion algorithm is designed to be robust against race conditions, especially in the rare but possible case of multiple identical fingerprints occupying the same candidate buckets. Each thread assigned a deletion task searches through its two candidate buckets for the target fingerprint. Upon finding a potential match, the thread performs an atomic compare-and-swap (CAS) operation to replace the fingerprint with an "empty" marker (zero).

The outcome of this atomic operation dictates the thread's next action:

\begin{itemize}
  \item \textbf{If the CAS succeeds}, the thread has successfully located and removed its target. The deletion is considered complete, and the thread terminates its search.
  \item \textbf{If the CAS fails}, it implies that another thread has modified that specific slot in the brief moment after it was found (e.g., by deleting a different item that happened to have the same fingerprint). In this event, the thread's deletion is not yet complete, and it must continue its linear scan through the rest of the bucket(s) to find another instance of the fingerprint to attempt to remove.
\end{itemize}

This continues until a deletion is successful, or both buckets have been fully traversed. This ensures that each deletion request is handled independently and correctly, even in the presence of duplicate fingerprints. The success or failure of each operation can be optionally reported back to the host via a provided bit-vector. Similarly to how it is done after insertion, a hierarchical reduction on the success values is used to minimize contention when decrementing the atomic counter that holds the number of occupied slots in the filter.

% tex-fmt: off
\begin{algorithm}[H]
  \caption{Parallel Deletion}
  \label{alg:parallel-deletion}
\begin{algorithmic}[0]
  \Function{Remove}{key}
    \State $h = \mathrm{hash}(\text{key})$
    \State $fp = \mathrm{upper32}(h) \mathbin{\&} \bigl((1 \ll \text{bitsPerTag}) - 1\bigr)$
    \State $i_1 = \mathrm{lower32}(h) \bmod \text{numBuckets}$
    \State $i_2 = i_1 \oplus \mathrm{hash}(fp) \bmod \text{numBuckets}$
    \State \textbf{return} TryRemove(bucket $i_1$, $fp$) \textbf{or} TryRemove(bucket $i_2$, $fp$)
  \EndFunction
  \Statex
  \Function{TryRemove}{bucket, tag}
    \State startWord $= (\text{tag} \bmod \text{bucketSize}) / \text{tagsPerWord}$
    \For{$i = 0$ \textbf{to} wordCount - 1}
        \State currWord $= (\text{startWord} + i) \bmod \text{wordCount}$
        \For{$j = 0$ \textbf{to} tagsPerWord - 1}
            \If{tag at slot $j$ in currWord $==$ tag}
                \If{atomically replace tag at slot $j$ with EMPTY}
                    \State \textbf{return} \texttt{Success}
                \EndIf
            \EndIf
        \EndFor
    \EndFor
    \State \textbf{return} \texttt{Failure}
  \EndFunction
\end{algorithmic}
\end{algorithm}
% tex-fmt: on

\section{Optimization Techniques}
\label{sec:implementation:optimisation-techniques}

Beyond the core algorithm design, some noteworthy optimization strategies were explored to further improve insertion performance, particularly in cases where the filter is too large to fit into the device's L2 cache or under high load factors.

\subsection{Sorted Insertion}
\label{sec:implementation:optimisation-techniques:sorted}

One version of the insertion algorithm was implemented to enhance memory locality. Before the main insertion kernel is launched, the items are first packed into a temporary structure where the upper bits represent the primary bucket index and the lower bits contain the fingerprint. This array is then sorted in parallel on the GPU using CUB's high-performance radix sort. Doing this ensures that consecutive threads in the subsequent insertion kernel are likely to be working on items that map to the same or nearby buckets. This approach led to a measurable performance increase once the filter size exceeded the GPU's L2 cache, as the random memory accesses of the standard approach were more heavily penalized. However, for filters that fit entirely within the L2 cache (which has a hit rate of over 90\% in these cases), the overhead of the initial packing and sorting pass made this version considerably slower and not worth using.

\subsection{Alternative Eviction Strategy}
\label{sec:implementation:optimisation-techniques:eviction}

The standard eviction process uses a greedy, depth-first-search (DFS) approach, where a thread immediately follows the eviction chain of a single displaced item. An alternative strategy was implemented to reduce the average length of these chains. When an eviction is necessary, instead of picking one random item to displace, the thread first inspects a configurable number of candidate fingerprints from the bucket (e.g., half the bucket size). For each candidate, it checks if its alternate bucket has an empty slot. If such a "safe" eviction is found, the swap is performed, and the insertion completes in a single step. If all candidates lead to full alternate buckets, the algorithm falls back to the original DFS-style greedy eviction. This method was found to have a negligible impact when inserting into an empty filter, but it provided a moderate speed-up of 15-20\% when inserting into a filter with a high load factor (e.g., 70\% or more), where long eviction chains are more common.

\section{IPC Wrapper}
\label{sec:implementation:ipc-wrapper}

To allow the GPU-accelerated Cuckoo filter to be used as a high-performance, system-wide service, an Inter-Process Communication (IPC) wrapper was developed. This wrapper exposes the filter's functionality through a client-server architecture, with a core focus on enabling true zero-copy data transfer for maximum efficiency.

The architecture is built upon two primary mechanisms: a shared memory queue for communication and CUDA's IPC API for data access. A fixed-size ring buffer, chosen for its superior performance and simplicity in a concurrent environment, is created in a POSIX shared memory segment to act as a command queue. The key is how input data is handled. Instead of sending buffers through the queue, a client performs the following steps:

\begin{enumerate}
  \item Allocate memory in its own GPU memory space for the batch of items to be processed.
  \item Obtain opaque memory handles to this device memory using \\
    \texttt{cudaIpcGetMemHandle}.
  \item Place these handles inside a request message alongside other metadata like the size of input and output buffers, which is then enqueued into the shared memory ring buffer.
\end{enumerate}

The server runs a single worker thread that continuously dequeues requests from this ring buffer. For each request, the server's thread uses \\ \texttt{cudaIpcOpenMemHandle} on the provided handle to obtain a direct, valid device pointer into the \textit{client's} GPU memory space. This allows the server to launch its kernels and read the input keys directly, completely avoiding the significant overhead of inter-process data copies.

The queue is blocking by design, meaning a client attempting to enqueue a request into a full queue will wait until a slot becomes available. The server supports both a graceful shutdown, where it stops accepting new requests but processes all outstanding items, and a forced shutdown that cancels all pending requests.

To simplify its use, a client library abstracts away the low-level IPC details. An optional Thrust wrapper is also provided to allow for seamless interaction with \texttt{thrust::device\_vector}. This architecture could be enhanced in several ways in the future:

\begin{itemize}
  \item The blocking communication model could be evolved into a fully asynchronous interface, inspired by modern APIs like Linux's \texttt{io\_uring}, to maximize throughput in high-concurrency scenarios.
  \item Support could be added for multiple worker threads managing filters on different GPUs to enable further load balancing.
  \item The filter could be exposed over a network by leveraging technologies like RDMA (Remote Direct Memory Access) to preserve the zero-copy transfer properties.
\end{itemize}

\section{Multi-GPU}
\label{sec:implementation:multi-gpu}

To handle datasets that exceed the memory capacity of a single graphics card, a multi-GPU version of the Cuckoo filter was implemented. This version transparently partitions the data and workload across all available devices, presenting a unified interface to the user. This approach, however, necessitates a shift in the public API, as the input and output buffers must now reside in host memory to accommodate their potentially massive size.

\subsection{Architectural Design}
\label{sec:implementation:multi-gpu-arch}

The core design choice was to instantiate a completely independent Cuckoo filter instance on each GPU, each managing its own dedicated device memory. While a single, logically distributed filter was considered, the practical overhead of managing eviction chains across the high-latency interconnect between GPUs was deemed prohibitive, as it would severely compromise insertion performance.

To distribute items across these independent filters, a deterministic partitioning scheme is used. Each key is assigned to a specific GPU based on the result of $hash(key) \bmod n$, where $n$ is the number of GPUs. While a uniform hash function would distribute keys evenly, minor variances are expected in practice. To account for this and prevent premature insertion failures on one device, each GPU's filter is allocated slightly more capacity (2\%) than its proportional share.

\subsection{Data Distribution and Processing}
\label{sec:implementation:multi-gpu-distribution}

Distributing the input data from the host to the correct GPU partition is a critical and complex challenge. A naive approach involving a single "primary" GPU that partitions and distributes all data creates a severe load imbalance, leaving other GPUs idle. Therefore, a fully parallelized, multi-stage workflow was implemented:

\begin{enumerate}
  \item \textbf{Initial Data Scatter}: The host-side input array is processed in large chunks. Each GPU copies a distinct and proportional sub-chunk from the host into its own device memory in parallel.

  \item \textbf{Local Partitioning}: Each GPU processes its local sub-chunk, hashing each key to determine its target GPU partition.

  \item \textbf{Communication and Exchange}: To efficiently exchange data, the NVIDIA Collective Communications Library (NCCL) is utilized. The GPUs first perform a collective communication to share the sizes of each partition they hold. This allows every GPU to allocate a sufficiently large buffer to receive all the keys destined for it. Following this, a single, highly optimized All-to-All communication is performed, where each GPU sends its locally computed partitions to their target GPUs. NCCL automatically leverages the fastest available interconnects (like NVLink) to make this exchange as efficient as possible.

  \item \textbf{Parallel Processing}: Once the All-to-All exchange is complete, each GPU holds all the keys that belong to its partition. At this point, each GPU can proceed to execute the standard single-GPU insertion, lookup, or deletion kernel on its local data, fully in parallel with the other devices.
\end{enumerate}

\subsection{Result Consolidation}
\label{sec:implementation:multi-gpu-results}

For lookup and deletion operations that require an output array, the results generated on each GPU must be consolidated and reordered to match the original input order. To solve this efficiently, the results are sent back to the GPUs that originally held the keys (pre-partitioning). This allows each GPU to perform a parallel scatter operation, writing its portion of the results into the final host output buffer in the correct order. This avoids numerous small, inefficient memory copies and preserves the parallelism of the overall process.

\subsection{Challenges and Overheads}
\label{sec:implementation:multi-gpu-challenges}

The multi-GPU implementation, while powerful, introduces its own set of overheads. The data partitioning stage on each GPU requires at least one sorting pass, a reduction, and an exclusive prefix-sum to manage the data. These operations are accelerated using the Thrust library, which itself requires significant temporary storage. This increased memory pressure on each GPU means that the processable chunk size must be carefully managed to avoid running out of device memory during the partitioning phase.

\section{Testing and Verification}
\label{sec:implementation:testing-and-verification}

\blindtext
