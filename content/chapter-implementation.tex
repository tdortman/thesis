\chapter{Implementation}
\label{sec:implementation}

\section{Overview}
\label{sec:implementation:overview}

\blindtext

\section{Data Structure Design}
\label{sec:implementation:data-structure-design}

The design of the GPU-accelerated Cuckoo filter emphasizes compile-time configuration and a memory layout optimized for parallel access patterns. This section details these design choices, from the high-level configuration structure down to the public-facing API.

\subsection{Compile-Time Configuration}
\label{sec:implementation:data-structure-design:comptime-config}

To maximize performance by allowing the compiler to generate highly specialized code, the filter's core parameters are defined at compile-time as template arguments. These parameters are consolidated into a single configuration structure, \texttt{CuckooConfig}, which provides a clean and explicit way to instantiate the filter. The primary configuration options are:

\begin{itemize}
  \item \texttt{bitsPerTag}: Defines the size of each fingerprint in bits. The implementation is specialized for values of 8, 16, or 32, which align well with standard integer types.

  \item \texttt{bucketSize}: Specifies the number of fingerprints stored in each bucket. A smaller bucket size generally leads to a lower false positive rate but can negatively impact insertion performance and overall occupancy. A default of 16 was chosen through empirical performance testing.

  \item \texttt{maxEvictions}: Sets the maximum number of displacements a single thread is allowed to perform during an insertion attempt before it gives up and reports a failure. The default value of 500 was determined empirically.

  \item \texttt{blockSize}: Configures the thread block size for the internal CUDA kernels. This parameter can be tuned to optimize GPU occupancy for different hardware architectures.

  \item \texttt{HashStrategy}: A template parameter that accepts a strategy class. This class encapsulates the logic for hashing items, calculating the required number of buckets, and deriving an item's alternate bucket from its primary one. The default implementation uses the standard XOR-based partial-key cuckoo hashing, but an alternative "Additive and Subtractive Cuckoo Filter" strategy has also been implemented to demonstrate this flexibility.
\end{itemize}

Throughout the implementation, \texttt{static\_assert} statements are used to enforce critical invariants at compile time, such as ensuring that certain parameters are powers of two to allow for efficient bitwise AND operations instead of more costly modulo arithmetic.

\subsection{Memory Layout and State Management}
\label{sec:implementation:mem-layout-state}

The filter's primary data storage is a single, contiguous linear array of fixed-size buckets allocated in the GPU's global memory. To maximize memory bandwidth and avoid misaligned access, the internal layout is carefully structured:

\begin{itemize}
  \item Each bucket is composed of an array of 64-bit unsigned integer words.

  \item Fingerprints (\texttt{tags}) are tightly packed within these 64-bit words. For example, a 64-bit word can hold eight 8-bit fingerprints or four 16-bit fingerprints.
\end{itemize}

This packed layout necessitates the use of bitwise shift and mask operations to extract individual fingerprints. However, the computational cost of these operations is negligible compared to the latency of memory access, making this a highly beneficial trade-off.

The filter's state is maintained by a single global atomic counter that tracks the total number of occupied slots. This counter resides in device memory and is updated atomically by the insertion and deletion kernels. Its value is only lazily copied to the host when an explicit query is made, minimizing host-device communication.

Due to a limitation of CUDA C++ where kernels cannot be class members, the implementation passes a pointer to the filter's class instance (\texttt{this}) to each launched kernel. This allows the threads to access the filter's configuration and state, such as data pointers and capacity, as needed.

\subsection{Public API}
\label{sec:implementation:api}

The filter exposes a comprehensive public interface for easy integration. Two sets of APIs are provided: a "traditional" C-style API that operates on raw device pointers and item counts, and, if the Thrust library is available, a set of convenience wrappers that accept \texttt{thrust::device\_vector} objects.

\subsubsection{\texttt{explicit CuckooFilter(size\_t capacity)}}

The constructor takes a single argument for the desired capacity in terms of the number of items. It guarantees that at least this many items can be stored. Depending on the chosen \texttt{HashStrategy}, the actual allocated capacity may be larger. For example, the default XOR-based strategy requires the number of buckets to be a power of two, so the requested capacity is rounded up to the next suitable size.

\subsubsection{\texttt{size\_t insertMany(const T* d\_keys, const size\_t n)}}

This function attempts to insert a batch of \texttt{n} items from a buffer in device memory pointed to by \texttt{d\_keys}. It is the caller's responsibility to ensure the pointer is valid and the buffer is sufficiently large. The function returns the updated total number of occupied slots in the filter after the insertion attempt.

\subsubsection{\texttt{size\_t insertManySorted(const T* d\_keys, const size\_t n)}}

This variant offers an alternative insertion strategy focused on improving memory locality. Before insertion, it first computes each item's primary bucket index and packs it with its fingerprint. This array of packed tags is then sorted on the GPU using CUB's radix sort. This ensures that threads with adjacent indices are highly likely to be accessing the same memory region. This approach generally outperforms the standard \texttt{insertMany} when the filter is too large to fit into the GPU's L2 cache. However, it incurs higher peak memory usage and is slower for smaller, cache-resident filters due to the overhead of the packing and sorting steps.

\subsubsection{\texttt{void containsMany(const T* d\_keys, const size\_t n, bool* d\_output)}}

Performs a batch lookup for \texttt{n} items. The results are written to the \texttt{d\_output} device buffer, where the boolean at each index corresponds to whether the item at the same index in \texttt{d\_keys} was found.

\subsubsection{\texttt{size\_t deleteMany(const T* d\_keys, const size\_t n, bool* d\_output = nullptr)}}

Performs a batch deletion of \texttt{n} items. If the optional \texttt{d\_output} buffer is provided, the success or failure of each individual deletion is reported in the same manner as the lookup operation.

\subsubsection{\texttt{void clear()}}

Resets the internal state of the filter by setting all buckets and the occupancy counter to zero. This operation does not deallocate any device memory.

\subsubsection{\texttt{float loadFactor()}}

Returns the current load factor of the filter, calculated as the number of occupied slots divided by the total capacity.

\subsubsection{\texttt{size\_t countOccupiedSlots()}}

A debugging utility that provides a ground-truth count of occupied slots. It operates by copying the entire filter data to the host and manually counting every non-zero fingerprint. As this is an extremely slow, synchronous operation, it should only be used for verification purposes if the internal atomic counter is suspected to be inaccurate.

\section{Parallel Algorithms}
\label{sec:implementation:parallel-algorithms}

The core of this thesis is the design of parallel algorithms for the Cuckoo filter's primary operations: insertion, lookup, and deletion. These algorithms are designed to be launched as CUDA kernels, where many threads cooperate to process batches of items simultaneously.

\subsection{Insertion}
\label{sec:implementation:insertion}

The parallel insertion algorithm is designed to handle a large batch of items in parallel, with each CUDA thread being responsible for inserting a single item. The process for each thread is as follows:

\begin{enumerate}
  \item \textbf{Hashing and Key Generation}: Each item is first hashed into a 64-bit value using the xxHash64 algorithm, chosen for its high speed and excellent statistical properties. This hash is then split: the upper 32 bits serve are used to derive the item's fingerprint, while the lower 32 bits are used as the basis for its primary bucket index. A crucial initial finding was that using the same bits for both the fingerprint and the primary index led to a high probability of identical fingerprints clustering in the same buckets, causing a severe degradation in performance. Thus, distinct parts of the hash are used for each. The alternate bucket index is then calculated using the partial-key cuckoo hashing scheme, which is described in detail in Section \ref{sec:background:cuckoo-filter}.

  \item \textbf{Direct Insertion Attempt}: The thread first checks the two candidate buckets for an empty slot. The thread uses the item's fingerprint to generate a random starting offset within each bucket and performs a linear search from that point. If an empty slot is found in either bucket, the fingerprint is inserted via an atomic operation, and the insertion for that item succeeds.

  \item \textbf{Eviction Process}: If both candidate buckets are full, the thread initiates the cuckoo process. It randomly selects one of the two buckets and a random occupied slot within it. It then atomically replaces the fingerprint (\texttt{tag\_old}) in that slot with its own fingerprint (\texttt{tag\_new}). The evicted fingerprint, \texttt{tag\_old}, now becomes the item that the thread must insert. The thread calculates \texttt{tag\_old}'s alternate bucket and continues this process in a loop.

  \item \textbf{Termination}: The eviction loop continues until either an empty slot is found or a predefined limit on the number of displacements is reached, at which point the insertion is reported as a failure.

\end{enumerate}

To maintain an accurate count of the total items in the filter without creating a bottleneck on a single atomic counter, a hierarchical reduction is employed. Each thread that successfully inserts an item contributes a +1. These values are first summed efficiently at the warp level using shuffle instructions, then aggregated at the block level using shared memory, and finally, a single atomic addition per block is performed on the global counter in device memory.

% FIXME: I don't really like this formatting but that's okay, can always change it later
% tex-fmt: off
\begin{algorithm}[H]
  \caption{Parallel Insertion}
  \label{alg:parallel-insertion}
\begin{algorithmic}[0]
  \Function{Insert}{key}
    \State $h = \mathrm{hash}(\text{key})$
    \State $fp = \mathrm{upper32}(h) \mathbin{\&} \bigl((1 \ll \text{bitsPerTag}) - 1\bigr)$
    \State $i_1 = \mathrm{lower32}(h) \bmod \text{numBuckets}$
    \State $i_2 = i_1 \oplus \mathrm{hash}(fp) \bmod \text{numBuckets}$
    \If{there is an empty slot in bucket $i_1$ or $i_2$}
      \State insert $fp$ into an empty slot (atomic)
      \State \textbf{return} \texttt{Success}
    \Else
      \State $\text{tag} = fp$
      \State $b =$ randomly pick $i_1$ or $i_2$
      \For{$n = 1$ \textbf{to} \text{maxEvictions}}
        \State $s =$ random slot in bucket $b$
        \State $\text{tag} = \text{atomic\_exchange}(buckets[b][s],\ \text{tag})$
        \State $b = \text{alt\_bucket}(b, \text{tag})$
        \If{there is an empty slot in bucket $b$}
          \State insert \text{tag} into an empty slot (atomic)
          \State \textbf{return} \texttt{Success}
        \EndIf
      \EndFor
      \LComment{Filter is considered full}
      \LComment{Caller will have to increase its capacity}
      \State \textbf{return} \texttt{Failure}
    \EndIf
  \EndFunction
\end{algorithmic}
\end{algorithm}
% tex-fmt: on

\subsection{Lookup}
\label{sec:implementation:lookup}

The parallel lookup algorithm is similar to insertion but is a read-only operation, allowing for more aggressive memory access optimizations. Each thread is assigned an item to look for.

The thread calculates the item's fingerprint and its two candidate bucket indices, just as in the insertion process. It then searches both buckets for a matching fingerprint, again using the fingerprint to determine a random starting point for a linear probe.

The key optimization in the lookup kernel is the use of vectorized, non-atomic memory loads. Since it can be guaranteed that no other kernel is writing to the filter during a lookup operation, thread safety is not a concern. Modern GPU hardware can load 128 bits (or 16 bytes) in a single instruction. The kernel leverages this by loading two 64-bit words at a time, allowing a thread to check multiple fingerprints with a single memory transaction. As most lookups will find their item (or determine its absence) within the first few slots checked, this approach ensures that the average lookup requires only a single memory access per item, dramatically increasing throughput. The results of the lookup (found or not found) are written to a user-provided output array.

% tex-fmt: off
\begin{algorithm}[H]
  \caption{Parallel Lookup}
  \label{alg:parallel-lookup}
\begin{algorithmic}[0]
  \Function{Contains}{key}
    \State $h = \mathrm{hash}(\text{key})$
    \State $fp = \mathrm{upper32}(h) \mathbin{\&} \bigl((1 \ll \text{bitsPerTag}) - 1\bigr)$
    \State $i_1 = \mathrm{lower32}(h) \bmod \text{numBuckets}$
    \State $i_2 = i_1 \oplus \mathrm{hash}(fp) \bmod \text{numBuckets}$
    \State \textbf{return} FindSlot(bucket $i_1$, $fp$) \textbf{or} FindSlot(bucket $i_2$, $fp$)
  \EndFunction
  \Statex
  \Function{FindSlot}{bucket, tag}
    \State startSlot $=$ tag $\bmod$ bucketSize
    \State startWord $=$ startSlot / tagsPerWord
    \State startPair $=$ floorToEven(startWord)

    \For{$i = 0$ \textbf{to} wordCount / 2}
        \State pairIdx $=$ (startPair + i * 2) $\bmod$ wordCount
        \State word1, word2 $=$ loadTwoWordsAt(bucket, pairIdx)

        \For{each word \textbf{in} \{word1, word2\}}
            \For{$j = 0$ \textbf{to} tagsPerWord}
                \If{extractTag(word, $j$) == tag}
                    \State \textbf{return} \texttt{Success}
                \EndIf
            \EndFor
        \EndFor
    \EndFor
    \State \textbf{return} \texttt{Failure}
  \EndFunction
\end{algorithmic}
\end{algorithm}
% tex-fmt: on
\subsection{Deletion}
\label{sec:implementation:deletion}

The parallel deletion algorithm is designed to be robust against race conditions, especially in the rare but possible case of multiple identical fingerprints occupying the same candidate buckets. Each thread assigned a deletion task searches through its two candidate buckets for the target fingerprint. Upon finding a potential match, the thread performs an atomic compare-and-swap (CAS) operation to replace the fingerprint with an "empty" marker (zero).

The outcome of this atomic operation dictates the thread's next action:

\begin{itemize}
  \item \textbf{If the CAS succeeds}, the thread has successfully located and removed its target. The deletion is considered complete, and the thread terminates its search.
  \item \textbf{If the CAS fails}, it implies that another thread has modified that specific slot in the brief moment after it was found (e.g., by deleting a different item that happened to have the same fingerprint). In this event, the thread's deletion is not yet complete, and it must continue its linear scan through the rest of the bucket(s) to find another instance of the fingerprint to attempt to remove.
\end{itemize}

This continues until a deletion is successful, or both buckets have been fully traversed. This ensures that each deletion request is handled independently and correctly, even in the presence of duplicate fingerprints. The success or failure of each operation can be optionally reported back to the host via a provided bit-vector. Similarly to how it is done after insertion, we again utilize a hierarchical reduction on the success values to minimize contention when decrementing the atomic counter that holds the number of occupied slots in the filter.

% tex-fmt: off
\begin{algorithm}[H]
  \caption{Parallel Deletion}
  \label{alg:parallel-deletion}
\begin{algorithmic}[0]
  \Function{Remove}{key}
    \State $h = \mathrm{hash}(\text{key})$
    \State $fp = \mathrm{upper32}(h) \mathbin{\&} \bigl((1 \ll \text{bitsPerTag}) - 1\bigr)$
    \State $i_1 = \mathrm{lower32}(h) \bmod \text{numBuckets}$
    \State $i_2 = i_1 \oplus \mathrm{hash}(fp) \bmod \text{numBuckets}$
    \State \textbf{return} TryRemove(bucket $i_1$, $fp$) \textbf{or} TryRemove(bucket $i_2$, $fp$)
  \EndFunction
  \Statex
  \Function{TryRemove}{bucket, tag}
    \State startWord $= (\text{tag} \bmod \text{bucketSize}) / \text{tagsPerWord}$
    \For{$i = 0$ \textbf{to} wordCount - 1}
        \State currWord $= (\text{startWord} + i) \bmod \text{wordCount}$
        \For{$j = 0$ \textbf{to} tagsPerWord - 1}
            \If{tag at slot $j$ in currWord $==$ tag}
                \If{atomically replace tag at slot $j$ with EMPTY}
                    \State \textbf{return} \texttt{Success}
                \EndIf
            \EndIf
        \EndFor
    \EndFor
    \State \textbf{return} \texttt{Failure}
  \EndFunction
\end{algorithmic}
\end{algorithm}
% tex-fmt: on

\section{Optimization Techniques}
\label{sec:implementation:optimisation-techniques}

Beyond the core algorithm design, some noteworthy optimization strategies were explored to further improve insertion performance, particularly in cases where the filter is too large to fit into the device's L2 cache or under high load factors.

\subsection{Sorted Insertion}
\label{sec:implementation:optimisation-techniques:sorted}

One version of the insertion algorithm was implemented to enhance memory locality. Before the main insertion kernel is launched, the items are first packed into a temporary structure where the upper bits represent the primary bucket index and the lower bits contain the fingerprint. This array is then sorted in parallel on the GPU using CUB's high-performance radix sort. Doing this ensures that consecutive threads in the subsequent insertion kernel are likely to be working on items that map to the same or nearby buckets. This approach led to a measurable performance increase once the filter size exceeded the GPU's L2 cache, as the random memory accesses of the standard approach were more heavily penalized. However, for filters that fit entirely within the L2 cache (which has a hit rate of over 90\% in these cases), the overhead of the initial packing and sorting pass made this version considerably slower and not worth using.

\subsection{Alternative Eviction Strategy}
\label{sec:implementation:optimisation-techniques:eviction}

The standard eviction process uses a greedy, depth-first-search (DFS) approach, where a thread immediately follows the eviction chain of a single displaced item. An alternative strategy was implemented to reduce the average length of these chains. When an eviction is necessary, instead of picking one random item to displace, the thread first inspects a configurable number of candidate fingerprints from the bucket (e.g., half the bucket size). For each candidate, it checks if its alternate bucket has an empty slot. If such a "safe" eviction is found, the swap is performed, and the insertion completes in a single step. If all candidates lead to full alternate buckets, the algorithm falls back to the original DFS-style greedy eviction. This method was found to have a negligible impact when inserting into an empty filter, but it provided a moderate speed-up of 15-20\% when inserting into a filter with a high load factor (e.g., 70\% or more), where long eviction chains are more common.

\section{IPC Wrapper}
\label{sec:implementation:ipc-wrapper}

To allow the GPU-accelerated Cuckoo filter to be used as a high-performance, system-wide service, an Inter-Process Communication (IPC) wrapper was developed. This wrapper exposes the filter's functionality through a client-server architecture, with a core focus on enabling true zero-copy data transfer for maximum efficiency.

The architecture is built upon two primary mechanisms: a shared memory queue for communication and CUDA's IPC API for data access. A fixed-size ring buffer, chosen for its superior performance and simplicity in a concurrent environment, is created in a POSIX shared memory segment to act as a command queue. The key is how input data is handled. Instead of sending buffers through the queue, a client performs the following steps:

\begin{enumerate}
  \item Allocate memory in its own GPU memory space for the batch of items to be processed.
  \item Obtain opaque memory handles to this device memory using \\
    \texttt{cudaIpcGetMemHandle}.
  \item Place these handles inside a request message alongside other metadata like the size of input and output buffers, which is then enqueued into the shared memory ring buffer.
\end{enumerate}

The server runs a single worker thread that continuously dequeues requests from this ring buffer. For each request, the server's thread uses \\ \texttt{cudaIpcOpenMemHandle} on the provided handle to obtain a direct, valid device pointer into the \textit{client's} GPU memory space. This allows the server to launch its kernels and read the input keys directly, completely avoiding the significant overhead of inter-process data copies.

The queue is blocking by design, meaning a client attempting to enqueue a request into a full queue will wait until a slot becomes available. The server supports both a graceful shutdown, where it stops accepting new requests but processes all outstanding items, and a forced shutdown that cancels all pending requests.

To simplify its use, a client library abstracts away the low-level IPC details. An optional Thrust wrapper is also provided to allow for seamless interaction with \texttt{thrust::device\_vector}. This architecture could be enhanced in several ways in the future:

\begin{itemize}
  \item The blocking communication model could be evolved into a fully asynchronous interface, inspired by modern APIs like Linux's \texttt{io\_uring}, to maximize throughput in high-concurrency scenarios.
  \item Support could be added for multiple worker threads managing filters on different GPUs to enable further load balancing.
  \item The filter could be exposed over a network by leveraging technologies like RDMA (Remote Direct Memory Access) to preserve the zero-copy transfer properties.
\end{itemize}

\section{Testing and Verification}
\label{sec:implementation:testing-and-verification}

\blindtext
