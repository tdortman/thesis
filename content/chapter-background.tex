\chapter{Background and Related Work}
\label{sec:background}

Having established the motivation and objectives for designing a GPU-accelerated Cuckoo filter, this chapter provides the necessary background from its two foundational domains: probabilistic data structures and parallel computing. The discussion first traces the algorithmic lineage of the Cuckoo filter, beginning with the Bloom filter, progressing to the Cuckoo hashing scheme, and culminating in the Cuckoo filter itself. Subsequently, the focus shifts to the hardware and software paradigm with an introduction to the GPU architecture, the CUDA programming model, and the key performance considerations essential for developing efficient parallel algorithms. A firm grasp of both these areas is crucial for understanding the design choices and implementation challenges addressed in the remainder of this thesis.

\section{Probabilistic Data Structures}
\label{sec:background:probabilistic-data-structures}

At the heart of this thesis is the need for a dynamic and efficient data structure for approximate set membership. This section reviews the key structures that form the basis of the work presented. The analysis begins with the classic Bloom filter and its locality-optimized variant, which serve as important performance baselines. Following this is an explanation of the mechanics of Cuckoo hashing, the eviction-based strategy that enables the dynamic properties of the target data structure. Finally, the Cuckoo filter is detailed, showing how it combines these concepts to create a powerful and flexible alternative.

\subsection{Bloom Filter}
\label{sec:background:bloom-filter}

Invented in 1970, the Bloom filter has long been the dominant probabilistic data structure for approximate membership query (AMQ) problems. Its operation is based on a simple yet effective concept: a bit array of size $m$ and a set of $k$ independent hash functions. To insert an item, the item is hashed $k$ times, and each resulting hash value is used as an index to set a bit in the array to 1. To query for an item's membership, the item is again hashed $k$ times. If all corresponding bits in the array are 1, the item is considered to possibly be in the set. However, if even one bit is 0, the item is definitively not in the set, as illustrated in Figure \ref{fig:bloom-filter}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{images/bloom-filter.png}
  \caption{An illustration of a Bloom filter's insertion and lookup mechanism with $m=11$ slots and $k=3$ hash functions. The set $\{x, y, z\}$ has been inserted, setting the corresponding bits in the array to 1. A membership query for a new item $w$ returns a definitive "no" (guaranteeing no false negatives) because one of the bits it maps to is 0.}
  \label{fig:bloom-filter}
\end{figure}

The false-positive rate $\epsilon$ of a Bloom filter after inserting $n$ items is approximately:
\begin{equation}
  \epsilon \approx \left(1 - e^{-nk/m}\right)^k
  \label{eq:bloom-fpr}
\end{equation}

This rate is optimized by choosing the optimal number of hash functions, which for a given $n$ and $m$ is:
\begin{equation}
  k = \frac{m}{n} \ln 2
\end{equation}

Given a target false positive rate $\epsilon$, the number of hash functions used by a space-optimized Bloom filter is given by:

\begin{equation}
  k = \log_2(1/\epsilon)
\end{equation}

Such a filter uses $1.44 \log_2(1/\epsilon)$ bits per element.

Despite its widespread use, the classic Bloom filter has several notable disadvantages:

\begin{itemize}
  \item \textbf{No Deletion}: The standard implementation does not support the removal of items, as clearing a bit could inadvertently remove other items that hash to the same location. Variants like the Counting Bloom Filter address this by using counters instead of single bits, but at a significant cost to space efficiency.
  \item \textbf{Linear Complexity}: Lookup and insertion performance is dependent on the number of hash functions, which scales linearly with $m$. This can lead to performance bottlenecks in high-throughput scenarios.
  \item \textbf{Poor Memory Locality}: The $k$ hash functions produce indices that are typically scattered randomly across the entire bit array. This leads to poor cache performance on CPUs and is particularly detrimental on GPUs, where it prevents efficient memory access.
  \item \textbf{Degrading Performance}: As the filter fills up and more bits are set to 1, the false positive rate steadily increases, eventually converging to a point where all queries yield a positive result.
  \item \textbf{Suboptimal Space Usage}: The optimal space usage for a Bloom filter is approximately 44\% higher than the information theoretical lower bound for AMQ data structures. Many modern filters, including the Cuckoo filter, achieve a lower overhead relative to this bound.
\end{itemize}

\subsection{Blocked Bloom Filter}
\label{sec:background:blocked-bloom-filter}

To address the critical issue of poor memory locality, the Blocked Bloom Filter was introduced. As this variant is a primary point of comparison in the performance evaluation in this thesis, its design warrants a more detailed explanation.

Instead of a single monolithic bit array, the Blocked Bloom Filter partitions the array into an array of smaller, independent Bloom filters, called blocks. Each block is typically sized to fit within a single CPU cache line (e.g., 64 bytes). The hashing scheme is modified: a single hash function is first used to map an incoming item to a specific block. Then, the standard $k$ hash functions are used to set or check bits only within that selected block, as shown in Figure \ref{fig:blocked-bloom-filter}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{images/blocked-bloom-filter.png}
  \caption{An illustration of a Blocked Bloom Filter. Each item is first mapped to a single block (e.g., item $y$ maps to Block 0, while $x$ and $z$ map to Block 1). The $k$ hash functions then operate only within that selected block, improving memory locality. A query for item $w$, which maps to Block 0, returns a definitive "no" as one of its target bits is 0.}
  \label{fig:blocked-bloom-filter}
\end{figure}

The primary advantage of this design is a dramatic improvement in memory locality. All $k$ memory accesses for a single item are now confined to a small, contiguous memory region. This is highly beneficial for modern CPU caches and, more importantly for this thesis, is extremely well-suited to the parallel memory access patterns of GPUs. When multiple threads in a warp process items that map to the same block, their memory requests can be coalesced into a single transaction, significantly improving memory bandwidth utilization.

However, this performance gain comes at the cost of a higher false positive rate for the same amount of memory. By partitioning the filter, the Blocked Bloom Filter loses the "averaging" effect of the classic design. If an unlucky distribution of items causes one block to become heavily saturated, its local false positive rate will increase dramatically, and this "hotspot" can dominate the overall false positive rate of the entire structure. Therefore, the Blocked Bloom Filter represents a direct trade-off: it sacrifices some statistical efficiency for significantly better performance, making it a crucial and highly relevant baseline for evaluating cache-aware and GPU-accelerated data structures.

\subsection{Cuckoo Hashing}
\label{sec:background:cuckoo-hashing}

Cuckoo hashing, introduced by Pagh and Rodler in 2004 \cite{pagh2004cuckoo}, is a powerful hashing scheme that provides a significant advantage over many others: a worst-case $O(1)$ lookup time \cite{pagh2004cuckoo}. Its name is derived from the cuckoo bird, which is known for laying its eggs in the nests of other birds, often forcing the original occupants out. The scheme's elegance lies in its conceptual simplicity compared to previous methods that also offered worst-case constant lookup guarantees.

In its original formulation, the scheme uses two independent hash tables, $T_1$ and $T_2$, each of size $r$, and two independent hash functions $h_1$ and $h_2$. The core invariant of Cuckoo hashing is that for any given item $x$, it is stored in one of two possible locations: either $\mathrm{T}_1[\mathrm{h}_1(x) \bmod r]$ or $\mathrm{T}_2[\mathrm{h}_2(x) \bmod r]$. A lookup operation is therefore guaranteed to be $O(1)$, as it only requires checking these two specific locations.

The insertion process is more involved and follows the distinct "cuckoo" eviction protocol, as illustrated in Figure \ref{fig:cuckoo-hashing}. The steps are as follows:

\begin{enumerate}
  \item For a new item $x$, its two potential locations $i_1 = h_1(x) \bmod r$ and \\ $i_2 = h_2(x) \bmod r$ are computed.

  \item If either $\mathrm{T}_1[\mathrm{i}_1]$ or $\mathrm{T}_2[\mathrm{i}_2]$ is empty, the item is placed there, and the insertion is complete.

  \item If both slots are occupied, an existing item (say $y$) is evicted from one of the location (e.g. $i_1$) and $x$ is placed there.

  \item The evicted item $y$ is then reinserted into its alternate location. If $y$ was in $T_1$, it now attempts to move to $\mathrm{T}_2[\mathrm{i}_2]$. This move may, in turn, cause another eviction, leading to a chain of evictions.

  \item To prevent infinite loops, a maximum number of evictions is set. If this limit is reached, the table is considered full, and all items must be rehashed with a new pair of hash functions, potentially into larger tables.
\end{enumerate}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{images/cuckoo-hashing.png}
  \caption{Illustration of a Cuckoo hashing insertion in a single-table variant. The incoming item $x$ finds both of its candidate slots occupied. It evicts item $a$, triggering a cascading eviction chain until item $d$ is moved to a free slot, resulting in a new stable arrangement of items in the table.}
  \label{fig:cuckoo-hashing}
\end{figure}

The success of this probabilistic scheme hinges on the load factor. The theory guarantees that if the tables are kept sufficiently sparse. Specifically, if the number of slots in each table $r$ is greater than $(1+\epsilon)n$, where $n$ is the number of items, the probability of an insertion failing and requiring a rehash is very low \cite{pagh2001cell}. This effectively means the total load factor across both tables should be kept below 50\%.

A common and important variant, is to use a single table of size $m = 2r$. This is more space-efficient and is the model that more closely resembles a Cuckoo filter's implementation. This variant provides an important conceptual link to the Cuckoo filter because it introduces the idea of deriving an alternate location from a current one without storing any extra state.

Pagh and Rodler describe a trick, attributed to John Tromp, that achieves this. The key insight is to redefine the two potential locations for an item $x$. Instead of simply being $h_1(x) \bmod m$ and $h_2(x) \bmod m$, the locations are defined as:

\begin{itemize}
  \item $i_1 = h_1(x) \bmod m$
  \item $i_2 = (h_2(x) - h_1(x)) \bmod m$
\end{itemize}

With this specific construction, a symmetric, state-free mapping function can be used to jump between the two locations. If an item $x$ is currently at a location $i$, its alternate location $i'$ can be calculated using the single mapping

\begin{equation}
  i' = (h_2(x) - i) \bmod m
\end{equation}

This works because the transformation is its own inverse for that specific pair of locations. Applying it to $i_1$ yields $i_2$, and applying it again to $i_2$ yields $i_1$. This concept of a state-free, symmetric, and computable mapping is the direct precursor to the partial-key hashing scheme used in Cuckoo filters.

\subsection{Cuckoo Filter}
\label{sec:background:cuckoo-filter}

The Cuckoo filter, introduced by Fan et al., is a probabilistic data structure that adapts the principles of Cuckoo hashing to provide a highly space-efficient and performant alternative to the Bloom filter, most notably by supporting dynamic deletion of items \cite{og-cuckoo-filter}. It improves upon standard Cuckoo hashing with several important distinctions.

First, instead of storing entire items, a Cuckoo filter only stores a small fingerprint for each item, which is a fixed-size sequence of bits derived from the item's hash. This significantly reduces the memory footprint. A direct consequence, however, is that traditional rehashing is not an option, as the original items are not available to be re-inserted. Therefore, the filter's size and parameters are typically static once it is created.

Second, it replaces the standard Cuckoo hashing scheme with partial-key cuckoo hashing. This clever technique establishes a symmetric dependency between the two potential bucket locations and the item's fingerprint. A single hash function is used to compute an initial location $i_1$ and a fingerprint $f$. The alternate location $i_2$ is then derived using the XOR operation

\begin{equation}
  i_2 = i_1 \oplus \mathrm{hash}(f)
  \label{eq:partial-cuckoo-hashing}
\end{equation}

Thanks to the symmetric nature of XOR, this relationship works both ways, such that $i_1 = i_2 \oplus \mathrm{hash}(f)$. This allows an evicted fingerprint to calculate its alternate location from its current position and its own fingerprint alone, a crucial feature since the original item is not stored.

\subsubsection{Design and Performance Characteristics}
\label{sec:background:cuckoo-filter:design-perf}

The design of the Cuckoo filter leads to several practical advantages over Bloom filters and their variants.

\begin{itemize}
  \item \textbf{High Space Utilization}: By allowing items to be relocated during insertion, Cuckoo filters can achieve very high load factors. The paper demonstrates that with a bucket size of 4, the filter can consistently reach an occupancy of over 95\%. The GPU implementation in this thesis generally prefers larger bucket sizes for performance reasons, thus consistently reaching a load factor of over 99\%.

  \item \textbf{Space Efficiency vs. False Positive Rate}: The false positive rate $\epsilon$ of a Cuckoo filter is directly tied to the fingerprint size $f$ and the bucket size $b$. Because a lookup must check up to $2b$ fingerprints in the worst case, the false positive rate can be approximated by:

    \begin{equation}
      \epsilon \approx 2b/2^f
    \end{equation}

    The actual amortized space cost per item, $C$, is the fingerprint size $f$ divided by the filter's load factor $\alpha$ (e.g., $\alpha \approx 0.955$ for buckets of size 4), since the cost of the unoccupied slots must be distributed among the stored items. The minimal fingerprint size required to achieve a target rate $\epsilon$ is approximately

    \begin{equation}
      f \ge \log_2(2b/\epsilon) = \log_2(1/\epsilon) + \log_2(2b)
    \end{equation}

    By substituting this minimal required value for $f$ into the cost definition, an upper bound for the amortized space cost can be established:

    \begin{equation}
      C \le \frac{\log_2(1/\epsilon) + \log_2(2b)}{\alpha}
      \label{eq:cuckoo-space-bound}
    \end{equation}

    This equation is critical as it decomposes the cost into the information-theoretic lower bound ($\log_2(1/\epsilon)$), an overhead term related to the bucket size ($\log_2(2b)$), and an efficiency penalty from the load factor ($1/\alpha$). It clearly reveals the central trade-off: a larger bucket size $b$ improves the load factor $\alpha$ but also increases the overhead term, requiring careful tuning to optimize space. For a target $\epsilon < 3\%$, the authors show that a well-configured Cuckoo filter is more space-efficient than a space-optimized Bloom filter.

  \item \textbf{Performance}: A key performance advantage is that any lookup, positive or negative, always reads a fixed number of buckets, resulting in (at most) two cache line misses. This is a significant improvement over standard Bloom filters, where the number of memory probes $k$ scales with the desired false positive rate and can be much larger than two. This theoretical efficiency translates to strong practical performance. As demonstrated later in the evaluation in Section \ref{sec:eval:throughput}, while it does not fully match the raw throughput of the highly cache-optimized (and non-deletable) Blocked Bloom Filter, a well-managed GPU Cuckoo Filter is highly competitive and easily surpasses all other tested filters.
\end{itemize}

\section{GPU Computing}
\label{sec:background:gpu-computing}

This section introduces the fundamental concepts of Graphics Processing Unit (GPU) architectures and the CUDA programming model. An understanding of these principles is essential for contextualizing the design decisions and performance optimizations discussed in the subsequent implementation of the parallel Cuckoo filter. While this thesis targets NVIDIA GPUs using the CUDA framework, the core concepts of massively parallel processing are applicable to other GPU architectures and, to a large extent, modern multicore CPUs as well.

\subsection{The GPU as a Parallel Processor}
\label{sec:background:gpu-parallel-processor}

At its core, a GPU is a specialized processor designed for massive data parallelism. Originally developed to accelerate the computationally intensive task of rendering graphics, its architecture has evolved to become highly effective for general-purpose computing.

The primary architectural difference between a Central Processing Unit (CPU) and a GPU lies in their design philosophies. A CPU is optimized for low-latency execution of a single or a few sequential instruction streams (threads). It dedicates a significant portion of its silicon to sophisticated flow control and large data caches to minimize the execution time of a single task. In contrast, a GPU is designed for high-throughput computing. It makes up for slower single-thread performance by executing thousands of threads in parallel, dedicating far more of its transistors to data processing units (arithmetic logic units) rather than to data caching and flow control. Figure \ref{fig:cpu-vs-gpu} highlights this difference. For example, a modern NVIDIA Blackwell architecture GPU can feature over 24,000 cores. This design results in much higher instruction throughput and memory bandwidth, which is ideal for problems that can be broken down into many independent sub-problems.

Not all parts of a program can be effectively parallelized. The portions that can are typically isolated and rewritten as kernels, which are functions compiled separately for the GPU's instruction set. The typical workflow for a GPU-accelerated program is as follows:

\begin{enumerate}
  \item Allocate memory on the device (GPU).
  \item Copy input data from host memory (CPU's RAM) to device memory.
  \item Launch a kernel on the host, which is executed in parallel by many threads on the device.
  \item Copy the results from device memory back to host memory.
  \item Deallocate memory on the device.
\end{enumerate}

This data transfer overhead means that GPUs are most effective for problems where the computational work significantly outweighs the amount of data that needs to be transferred.

\begin{figure*}
  \centering
  \includegraphics[width=1\linewidth]{images/cpu-vs-gpu.png}
  \caption{The GPU Devotes More Transistors to Data Processing \cite{nvidia_cuda_guide}}
  \label{fig:cpu-vs-gpu}
\end{figure*}

\subsection{CUDA Programming Model}
\label{sec:background:cuda-programming-model}

To manage the immense parallelism of the hardware, NVIDIA developed CUDA (Compute Unified Device Architecture), a parallel computing platform and programming model. CUDA provides a set of extensions to the C++ language that allows developers to write host (CPU) and device (GPU) code within the same environment, simplifying the development process. It abstracts the hardware into a logical hierarchy that is more manageable for the programmer.

The basic unit of execution in CUDA is a thread. When a kernel is launched, it is executed by a vast number of these threads on the GPU. These threads are organized into a three-dimensional hierarchy:

\begin{itemize}
  \item \textbf{Threads}: The smallest execution unit. Each thread executes the same kernel code but operates on different data, identified by its unique coordinates within a block.
  \item \textbf{Blocks}: Each block can contain up to 1024 threads, which can work together by sharing data through fast on-chip shared memory and synchronizing their progress.
  \item \textbf{Grid}: Blocks are organized into a grid. All blocks in a grid execute the same kernel. Blocks are assumed to execute independently and in any order, and there is no guaranteed synchronization mechanism between them during a single kernel launch.\footnote{Newer cards support Cooperative Groups, which allow this to an extent}
\end{itemize}

This hierarchical grid structure makes it easy to map threads to data. For instance, when processing an image, one might assign a single thread to each pixel, group threads for a tile of the image into a block, and have the entire grid of blocks process the full image.

\subsection{Hardware Architecture}
\label{sec:background:hardware-architecture}

This logical hierarchy maps onto the physical hardware of the GPU. A GPU is composed of multiple Streaming Multiprocessors (SMs). A global scheduler assigns thread blocks to available SMs for execution, and a key challenge in GPU programming is keeping these SMs saturated with work.

Each SM is a powerful parallel processor in its own right, containing several key components:

\begin{itemize}
  \item \textbf{CUDA Cores}: The basic arithmetic logic units (ALUs) that perform integer and floating-point calculations.
  \item \textbf{Warp Schedulers}: Decides which group of threads gets to execute on each clock cycle.
  \item \textbf{Special Function Units (SFUs)}: Handle complex mathematical operations like trigonometry and square roots.
  \item \textbf{Register Files}: The fastest storage on the GPU, holding thread-specific data and intermediate results. While each thread has its own unlimited logical set of registers, the physical register file is a limited resource shared across all active threads on an SM.
  \item \textbf{Shared Memory/L1 Cache}: A low-latency memory space used for user-managed data sharing within a thread block.
  \item \textbf{Load/Store Units}: Manage the movement of data between memory spaces.
\end{itemize}

Threads are not only grouped into blocks but are also managed by the SM in groups of 32 called warps. A warp is the fundamental unit of scheduling on the GPU. All 32 threads in a warp execute in a Single-Instruction-Multiple-Thread (SIMT) fashion, meaning they run in lockstep and execute the same instruction at the same time on different data. This SIMT execution model is the root cause of the most important performance considerations in GPU programming.

\subsection{Memory Hierarchy}
\label{sec:background:memory-hierarchy}

Performance in GPU programming is directly linked to memory access patterns. A deep understanding of the memory hierarchy is crucial for writing efficient code, as the primary goal of many CUDA optimizations is to maximize the use of fast memory and minimize traffic to slower memory.

\begin{itemize}
  \item \textbf{Registers}: The fastest memory on the GPU. Each thread has its own private registers for its local variables.
  \item \textbf{Shared Memory}: A small, low-latency, user-programmable memory space shared by all threads within a single block. It is essential for intra-block communication and is primarily used as a user-managed cache to avoid redundant reads from the much slower global memory.
  \item \textbf{Global Memory}: The largest memory space on the GPU (the device's VRAM), accessible to every running thread. It has the highest latency and serves as the medium for data transfer between the host and the device. Access to global memory should be minimized and carefully organized whenever possible.
  \item \textbf{Constant and Texture Memory}: Small, read-only memory spaces that are cached and optimized for specific access patterns, such as when many threads in a warp read from the same location (constant memory) or from spatially local locations (texture memory).
\end{itemize}

The primary goal of many CUDA optimizations is to maximize the use of fast memory (registers and shared memory) and minimize traffic to the slower global memory.

\subsection{Performance Considerations}
\label{sec:background:performance-considerations}

The SIMT execution model of warps and the tiered memory hierarchy lead to several critical performance considerations.

Access to global memory is often the most significant performance bottleneck. To mitigate this, the GPU hardware attempts to coalesce memory requests. This is a technique to improve memory bandwidth utilization by servicing multiple logical memory reads from a warp in a single physical memory transaction. This is possible because DRAM technology fetches data in bursts. Modern NVIDIA GPUs have a cache line size of 128 bytes. If the 32 threads in a warp access 32 consecutive 4-byte words in global memory, this 128-byte request can be satisfied with a single DRAM burst, achieving maximum bandwidth. Conversely, if the memory accesses are scattered and random, the hardware may require up to 32 separate transactions, drastically reducing performance.

A similar principle applies to shared memory, which is organized into 32 memory banks. Each bank can service one request per cycle. If all 32 threads in a warp access data in different banks, the entire request can be satisfied in a single cycle. However, if multiple threads access the same bank, a bank conflict occurs, and the accesses are serialized, reducing throughput.

At a higher level, a modern GPU can execute multiple operations concurrently, such as copying data from the host while a kernel is running. A key technique for maximizing utilization is to use CUDA streams. A stream is essentially a queue of operations that are executed in order. By using multiple streams, a programmer can enqueue work on the GPU in smaller, independent chunks. This allows the GPU scheduler to overlap memory transfers with computation (latency hiding). For example, the GPU can be copying the next chunk of data from the host while simultaneously processing the current chunk, effectively hiding the latency of the PCIe bus transfer and keeping the compute cores busy.

The physical resources on each SM are finite, and their management directly impacts performance. While the programming model exposes what seems like a nearly unlimited number of registers to each thread, in reality, there is a limited, physical register file that all threads active on an SM must share. High register usage by a kernel can have two detrimental effects. Firstly, if a single thread requires more registers than the hardware can allocate, some of its variables will "spill" into the much slower global memory, incurring high latency penalties on every access. Secondly, even if there is no spilling, if the collective register demand of all threads in a block is too high, the hardware scheduler will be forced to launch fewer concurrent warps on the SM. This reduction in occupancy (the ratio of active warps to the maximum supported warps) hurts the SM's ability to hide memory latency by switching to other warps, leading to lower overall utilization.

Finally, any operation that forces parallel threads to execute sequentially undermines the benefits of parallelism and degrades performance. Branch divergence is a classic example: since all threads in a warp execute the same instruction, if-else statements can cause serialization. If threads in a warp take different paths, the hardware executes each path serially while idling the threads on the other path, negating the benefit of parallelism. A similar and often more severe bottleneck arises from atomic contention. When many threads in a block attempt to perform an atomic operation on the same memory location simultaneously, the hardware is forced to serialize these requests. In cases of extreme contention, the performance can degrade to the point where a traditional lock-based critical section might even been faster. Therefore, minimizing both control-flow divergence and high-contention atomic operations is critical for writing efficient, scalable GPU code.

\section{Related Work}
\label{sec:related-work}
\subsection{Two Choice Filter}
\label{sec:related-work:two-choice-filter}

Recent work by McCoy et al. introduced the Two-Choice Filter (TCF), a data structure designed specifically for high-throughput, parallel execution on GPUs \cite{tcf}. The TCF shares the same high-level goal as the GPU-accelerated Cuckoo filter presented in this thesis: to provide a deletable, space-efficient filter that is optimized for the architectural constraints of a GPU.

Structurally, the TCF is similar to a Cuckoo filter. It organizes fingerprints into blocks that are sized to fit within a GPU cache line to ensure high memory locality. Furthermore, unlike a Cuckoo filter, it uses two hash functions to map each item to two candidate blocks.

The fundamental difference lies in the insertion strategy. Whereas a Cuckoo filter employs an eviction-based scheme, the TCF uses a simpler placement-based strategy derived from the "power-of-two-choices" paradigm to help with load balancing \cite{potc}. The insertion logic is as follows:

\begin{itemize}
  \item To insert an item, the TCF inspects both candidate blocks.
  \item The new fingerprint is then placed in the block that is currently less full.
  \item Crucially, there is no kicking or eviction. If both candidate blocks are completely full, the insertion fails.
\end{itemize}

This approach avoids the complex, potentially sequential logic of managing eviction chains, which is a major challenge when parallelizing a Cuckoo filter. However, it introduces a new problem: insertions can fail long before the filter reaches a high load factor. To overcome this, the TCF introduces a backing store, a small secondary hash table that stores the items that could not be placed in the main table \cite{tcf}. This two-level architecture allows the TCF to maintain a high overall occupancy while keeping the insertion logic in the primary store very simple. For intra-block operations, the TCF leverages CUDA Cooperative Groups to coordinate threads within a warp. This enables efficient, lock-free insertions and queries within a single block.

In summary, the Two-Choice Filter represents a different design trade-off for achieving the same goal. It prioritizes a simpler, non-evicting parallel insertion logic at the cost of requiring an additional backing data structure to handle overflows. This contrasts with the Cuckoo filter's approach of using a more complex, but single-structure, eviction-based algorithm.

\subsection{Quotient Filter}
\label{sec:related-work:quotient-filter}

The Quotient Filter (QF) is another modern, high-performance probabilistic data structure that, like the Cuckoo filter, improves upon the Bloom filter by supporting dynamic deletions and offering superior space efficiency in many common configurations \cite{og-qf}. It compactly stores small fingerprints of items in a set using a scheme based on Robin Hood hashing \cite{robin-hood-hashing}. For a target false positive rate $\epsilon$, a QF uses approximately $1.053(2.125 + \log_2(1/\epsilon))$ bits per item \cite{tcf}. This makes it more space-efficient than a space-optimized Bloom filter whenever $\epsilon \le 1/64$, a condition met by a wide range of practical applications \cite{tcf}.

The core mechanism of a QF is different from the eviction-based strategy of Cuckoo hashing. A $p$-bit hash of an item is split into a $q$-bit quotient and an $r$-bit remainder. The quotient determines an item's "canonical slot" in a table of $2^q$ slots, while the remainder is the value actually stored. If the canonical slot is occupied, the filter uses linear probing to find the next empty slot. All remainders sharing the same quotient form a contiguous run, and sequences of runs form clusters. Three metadata bits per entry \texttt{(is\_occupied, is\_continuation, is\_shifted)} are used to encode the structure of these runs, allowing for the reconstruction of the original hash during a lookup.

From a GPU design perspective, the Quotient Filter presents a compelling but challenging trade-off. Its linear-probing nature results in high cache locality, which is an appropriate choice for achieving the high memory coherence that GPUs favour.

However, its primary drawback is the insertion process. Inserting a new remainder may require shifting a long sequence of subsequent remainders to maintain the integrity of a run or cluster. This shifting operation is difficult to parallelize efficiently, makes it hard to use simple atomic operations, and results in high thread divergence, all of which are detrimental to GPU performance.

The challenge of adapting this structure to a parallel architecture was first addressed by Geil et al. \cite{gpu-qf}. Their work explored methods for parallelizing the QF, with a particular focus on the complex bulk build operation, which they identified as requiring a non-trivial parallel scan with a non-associative operator. However, this preliminary implementation has several significant limitations:

\begin{itemize}
  \item It was adapted from an earlier version of the Quotient filter and lacks modern features like counting.

  \item It suffers from higher space overhead compared to more recent designs.

  \item The implementation has specific constraints, such as a fixed false-positive rate and a maximum size of $2^{26}$ items, which result in poor performance and limited scalability for larger, more demanding applications.
\end{itemize}

Thus, while the Quotient filter is a powerful and flexible data structure in theory, its efficient implementation on a GPU remains a significant challenge, particularly with respect to its complex and inherently sequential insertion mechanics.