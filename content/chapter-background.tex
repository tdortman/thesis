\chapter{Background and Related Work}

\section{Probabilistic Data Structures}
\label{sec:background:probabilistic-data-structures}

Before diving into the design and implementation of the GPU-accelerated Cuckoo Filter, it is essential to establish a solid theoretical foundation. This chapter provides a comprehensive overview of the key data structures and algorithms that form the basis of this thesis. The discussion begins with the classic Bloom filter, the long-standing benchmark in approximate membership querying. We then introduce Cuckoo hashing, the foundational eviction scheme that gives the Cuckoo filter its name and unique properties. Finally, we detail the Cuckoo filter itself, explaining how it adapts these principles to create a more flexible and often more space-efficient alternative. Understanding these concepts is crucial for contextualizing the design choices and performance trade-offs discussed in the subsequent chapters.

\subsection{Bloom Filters}
\label{sec:background:bloom-filters}

Invented in 1970, the Bloom filter has long been the dominant probabilistic data structure for approximate membership query (AMQ) problems. Its operation is based on a simple yet effective concept: a bit array of size $m$ and a set of $k$ independent hash functions. To insert an item, the item is hashed $k$ times, and each resulting hash value is used as an index to set a bit in the array to 1. To query for an item's membership, the item is again hashed $k$ times. If all corresponding bits in the array are 1, the item is considered to possibly be in the set. However, if even one bit is 0, the item is definitively not in the set.

For a given number of items $n$ and a filter size of $m$ bits, the optimal number of hash functions to minimize the false positive rate is given by
$$k = \frac{m}{n} \ln 2$$
and given a false positive rate $\epsilon$, a space-optimized Bloom filter uses
$$k = \log_2\!\left(\frac{1}{\epsilon}\right)$$
hash functions. Such a Bloom filter uses $1.44 \log_2(1/\epsilon)$ bits per element.

Despite its widespread use, the classic Bloom filter has several notable disadvantages:

\begin{itemize}
  \item \textbf{No Deletion}: The standard implementation does not support the removal of items, as clearing a bit could inadvertently remove other items that hash to the same location. Variants like the Counting Bloom Filter address this by using counters instead of single bits, but at a significant cost to space efficiency.
  \item \textbf{Linear Complexity}: Lookup and insertion performance is dependent on the number of hash functions, which scales linearly with the number of items. This can lead to performance bottlenecks in high-throughput scenarios.
  \item \textbf{Poor Memory Locality}: The $k$ hash functions produce indices that are typically scattered across the bit array, leading to poor cache performance. While variants like the Blocked Bloom Filter improve locality, they do so at the expense of a higher false positive rate.
  \item \textbf{Degrading Performance}: As the filter fills up and more bits are set to 1, the false positive rate steadily increases, eventually converging to a point where all queries yield a positive result.
  \item \textbf{Suboptimal Space Usage}: The optimal space usage for a Bloom filter is $44\%$ higher than the information theoretical lower bound. Many filters, like the Cuckoo filter, add a lower overhead to this lower bound.
\end{itemize}

\subsection{Cuckoo Hashing}
\label{sec:background:cuckoo-hashing}

Cuckoo hashing is a hashing scheme that provides $O(1)$ worst-case lookup time. Its name is derived from the cuckoo bird, which is known for laying its eggs in the nests of other birds, often forcing the original occupants out.

The scheme uses two independent hash functions, $h_1$ and $h_2$, which map any given item $x$ to two potential locations in a hash table. A lookup operation is therefore guaranteed to be $O(1)$, as one only needs to check the positions $h_1(x)$ and $h_2(x)$.

The insertion process is more involved and follows a distinct "cuckoo" eviction protocol:

\begin{enumerate}
  \item For a new item $x$, its two potential locations $i_1 = h_1(x)$ and $i_2 = h_2(x)$ are computed.
  \item If either location is empty, the item is placed there and the insertion is complete.
  \item If both slots are occupied, an existing item (say $y$) is evicted from one of the locations (e.g., $i_1$), and $x$ is placed there.
  \item The evicted item $y$ is then reinserted into its alternate location $i_2 = h_2(y)$. If that location is also occupied, the process repeats, potentially leading to a chain of evictions.
  \item To prevent infinite loops, a maximum number of evictions is set. If this limit is reached, the table is resized, and all items are rehashed.
\end{enumerate}

Deletion is as simple as a lookup, followed by marking the item's slot as free.

\subsection{Cuckoo Filters}
\label{sec:background:cuckoo-filters}

A Cuckoo filter is a probabilistic data structure that adapts the principles of Cuckoo hashing to provide a highly space-efficient and performant alternative to the Bloom filter. It improves upon Cuckoo hashing with several important distinctions.

First, instead of storing entire items, a Cuckoo filter only stores a small fingerprint for each item, which is a fixed-size chunk of bits from the item's hash. This significantly reduces the memory footprint. A direct consequence, however, is that rehashing is not an option, as the original items are not available. Therefore, the filter's size is static once created.

Second, it replaces the standard Cuckoo hashing scheme with partial-key cuckoo hashing. This clever technique establishes a dependency between the two potential bucket locations and the item's fingerprint. A single hash function is used to compute an initial location $i_1$ and a fingerprint $f$. The alternate location $i_2$ is then derived using the XOR operation:

$$i_2 = i_1 \oplus hash(f)$$

Thanks to the symmetric nature of XOR, this relationship works both ways, such that $i_1 = i_2 \oplus hash(f)$. This allows an evicted fingerprint to calculate its alternate location from its current position and fingerprint alone, without needing the original item or a second hash function.

Because the filter only stores fingerprints, it is possible for two different items to have the same fingerprint, leading to false positives. The probability of such a hash collision is $2^{-f}$, where $f$ is the number of bits in the fingerprint. In practice, the actual false positive rate is slightly higher. For efficiency, the filter is implemented not as a flat array of slots but as an array of fixed-size buckets. Since a lookup requires checking all fingerprints in two potential buckets in the worst case, the false positive rate has an upper bound of approximately $2b * 2^{-f}$, where $b$ is the number of fingerprints stored per bucket.

This reveals a fundamental trade-off in the design of a Cuckoo filters. Increasing the bucket size $b$ reduces the likelihood of insertion failures and allows for higher filter occupancy. However, a larger bucket size also increases the number of fingerprints that must be checked during a lookup, which in turn linearly increases the false positive rate. Therefore, the parameters of a Cuckoo filter must be carefully tuned to balance space utilization against the acceptable rate of false positives.

\section{GPU Computing}
\label{sec:background:gpu-computing}

This section introduces the fundamental concepts of Graphics Processing Unit (GPU) architectures and the CUDA programming model. An understanding of these principles is essential for contextualizing the design decisions and performance optimizations discussed in the subsequent implementation of the parallel Cuckoo filter. While this thesis targets NVIDIA GPUs using the CUDA framework, the core concepts of massively parallel processing are applicable to other GPU architectures and, to a large extent, modern multicore CPUs as well.

\subsection{The GPU as a Parallel Processor}
\label{sec:background:gpu-parallel-processor}

At its core, a GPU is a specialized processor designed for massive data parallelism. Originally developed to accelerate the computationally intensive task of rendering graphics, its architecture has evolved to become highly effective for general-purpose computing.

The primary architectural difference between a Central Processing Unit (CPU) and a GPU lies in their design philosophies. A CPU is optimized for low-latency execution of a single or a few sequential instruction streams (threads). It dedicates a significant portion of its silicon to sophisticated flow control and large data caches to minimize the execution time of a single task. In contrast, a GPU is designed for high-throughput computing. It makes up for slower single-thread performance by executing thousands of threads in parallel, dedicating far more of its transistors to data processing units (arithmetic logic units) rather than to data caching and flow control. Figure \ref{fig:cpu-vs-gpu} highlights this difference. For example, a modern NVIDIA Blackwell architecture GPU can feature over 24,000 cores. This design results in much higher instruction throughput and memory bandwidth, which is ideal for problems that can be broken down into many independent sub-problems.

Not all parts of a program can be effectively parallelized. The portions that can are typically isolated and rewritten as kernels, which are functions compiled separately for the GPU's instruction set. The typical workflow for a GPU-accelerated program is as follows:

\begin{enumerate}
  \item Allocate memory on the device (GPU).
  \item Copy input data from host memory (CPU's RAM) to device memory.
  \item Launch a kernel on the host, which is executed in parallel by many threads on the device.
  \item Copy the results from device memory back to host memory.
  \item Deallocate memory on the device.
\end{enumerate}

This data transfer overhead means that GPUs are most effective for problems where the computational work significantly outweighs the amount of data that needs to be transferred.

\begin{figure*}
  \centering
  \includegraphics[width=1\linewidth]{images/cpu-vs-gpu.png}
  \caption{The GPU Devotes More Transistors to Data Processing \cite{nvidia_cuda_guide}}
  \label{fig:cpu-vs-gpu}
\end{figure*}

\subsection{CUDA Programming Model}
\label{sec:background:cuda-programming-model}

To manage the immense parallelism of the hardware, NVIDIA developed CUDA (Compute Unified Device Architecture), a parallel computing platform and programming model. CUDA provides a set of extensions to the C++ language that allows developers to write host (CPU) and device (GPU) code within the same environment, simplifying the development process. It abstracts the hardware into a logical hierarchy that is more manageable for the programmer.

The basic unit of execution in CUDA is a thread. When a kernel is launched, it is executed by a vast number of these threads on the GPU. These threads are organized into a three-dimensional hierarchy:

\begin{itemize}
  \item \textbf{Threads}: The smallest execution unit. Each thread executes the same kernel code but operates on different data, identified by its unique coordinates within a block.
  \item \textbf{Blocks}: Threads are grouped into blocks. Threads within the same block can cooperate by sharing data through a fast, on-chip shared memory and can synchronize their execution.
  \item \textbf{Grid}: Blocks are organized into a grid. All blocks in a grid execute the same kernel. Blocks are assumed to execute independently and in any order, and there is no guaranteed synchronization mechanism between them during a single kernel launch.\footnote{Newer cards support Cooperative Groups, which allow this to an extent}
\end{itemize}

This hierarchical grid structure makes it easy to map threads to data. For instance, when processing an image, one might assign a single thread to each pixel, group threads for a tile of the image into a block, and have the entire grid of blocks process the full image.

\subsection{Hardware Architecture}
\label{sec:background:hardware-architecture}

This logical hierarchy maps onto the physical hardware of the GPU. A GPU is composed of multiple Streaming Multiprocessors (SMs). A global scheduler assigns thread blocks to available SMs for execution, and a key challenge in GPU programming is keeping these SMs saturated with work.

Each SM is a powerful parallel processor in its own right, containing several key components:

\begin{itemize}
  \item \textbf{CUDA Cores}: The basic arithmetic logic units (ALUs) that perform integer and floating-point calculations.
  \item \textbf{Warp Schedulers}: Decides which group of threads gets to execute on each clock cycle.
  \item \textbf{Special Function Units (SFUs)}: Handle complex mathematical operations like trigonometry and square roots.
  \item \textbf{Register Files}: The fastest storage on the GPU, holding thread-specific data and intermediate results. While each thread has its own unlimited logical set of registers, the physical register file is a limited resource shared across all active threads on an SM.
  \item \textbf{Shared Memory/L1 Cache}: A low-latency memory space used for user-managed data sharing within a thread block.
  \item \textbf{Load/Store Units}: Manage the movement of data between memory spaces.
\end{itemize}

Threads are not only grouped into blocks but are also managed by the SM in groups of 32 called warps. A warp is the fundamental unit of scheduling on the GPU. All 32 threads in a warp execute in a Single-Instruction-Multiple-Thread (SIMT) fashion, meaning they run in lockstep and execute the same instruction at the same time on different data. This SIMT execution model is the root cause of the most important performance considerations in GPU programming.

\subsection{Memory Hierarchy}
\label{sec:background:memory-hierarchy}

Performance in GPU programming is directly linked to memory access patterns. A deep understanding of the memory hierarchy is crucial for writing efficient code, as the primary goal of many CUDA optimizations is to maximize the use of fast memory and minimize traffic to slower memory.

\begin{itemize}
  \item \textbf{Registers}: The fastest memory on the GPU. Each thread has its own private registers for its local variables.
  \item \textbf{Shared Memory}: A small, low-latency, user-programmable memory space shared by all threads within a single block. It is essential for intra-block communication and is primarily used as a user-managed cache to avoid redundant reads from the much slower global memory.
  \item \textbf{Global Memory}: The largest memory space on the GPU (the device's VRAM), accessible to every running thread. It has the highest latency and serves as the medium for data transfer between the host and the device. Access to global memory should be minimized and carefully organized whenever possible.
  \item \textbf{Constant and Texture Memory}: Small, read-only memory spaces that are cached and optimized for specific access patterns, such as when many threads in a warp read from the same location (constant memory) or from spatially local locations (texture memory).
\end{itemize}

The primary goal of many CUDA optimizations is to maximize the use of fast memory (registers and shared memory) and minimize traffic to the slower global memory.

\subsection{Performance Considerations}
\label{sec:background:performance-considerations}

The SIMT execution model of warps and the tiered memory hierarchy lead to several critical performance considerations.

Access to global memory is often the most significant performance bottleneck. To mitigate this, the GPU hardware attempts to coalesce memory requests. This is a technique to improve memory bandwidth utilization by servicing multiple logical memory reads from a warp in a single physical memory transaction. This is possible because DRAM technology fetches data in bursts. Modern NVIDIA GPUs have a cache line size of 128 bytes. If the 32 threads in a warp access 32 consecutive 4-byte words in global memory, this 128-byte request can be satisfied with a single DRAM burst, achieving maximum bandwidth. Conversely, if the memory accesses are scattered and random, the hardware may require up to 32 separate transactions, drastically reducing performance.

A similar principle applies to shared memory, which is organized into 32 memory banks. Each bank can service one request per cycle. If all 32 threads in a warp access data in different banks, the entire request is satisfied in a single cycle. However, if multiple threads access the same bank, a bank conflict occurs, and the accesses are serialized, reducing throughput.

Since all threads in a warp execute the same instruction at the same time, control flow statements like if-else can degrade performance. If threads within a warp take different paths based on a condition, the warp experiences branch divergence. The hardware handles this by serializing execution: first, the threads that take one path execute while the others are idle, and then the threads that take the other path execute while the first group is idle. This serialization negates the benefit of parallelism within the warp and should be minimized.

\section{Related Work}
\label{sec:background:related-work}

\subsection{Two Choice Filter}
\subsection{Morton Filter}
\subsection{Quotient Filter}