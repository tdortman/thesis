\clearpage

\section{Scalability (Multi-GPU)}
\label{sec:eval:multigpu}

To evaluate the filter's ability to scale beyond a single device, a specific set of benchmarks was conducted on a multi-GPU server node. These tests focus on two scalability metrics: Strong Scaling (speeding up a fixed-size problem) and Weak Scaling (maintaining performance as problem size grows with hardware).

\subsection{Experimental Setup}
\label{sec:eval:multigpu:setup}

For the evaluation of multi-device scalability, the experimental setup was expanded with a fourth hardware configuration representing a high-end enterprise cluster node:

% TODO: Update the OS, driver & cuda versions when the nodes are available again
\begin{itemize}
  \item \textbf{System D (HBM2e/NVLink)}: An AMD EPYC 7713 (64 cores) paired with $8\times$ NVIDIA A100-SXM4 GPUs, each featuring 80 GB of HBM2e memory ($\approx$ 2 TB/s bandwidth). The GPUs are interconnected via NVLink 3.0, providing a total aggregate bandwidth of 600 GB/s. The system runs AlmaLinux 10.1 with NVIDIA driver 580.95.05 and CUDA 12.9.86.
\end{itemize}

Unlike the PCIe-based tests performed on Systems A, B, and C, this configuration allows the Gossip library to leverage the high-bandwidth NVLink interconnect for the All-to-All data exchange, significantly reducing the communication latency during the partitioning phase.

\subsection{Strong Scaling}
\label{sec:eval:multigpu:strong}

Strong scaling measures how the execution time changes as the number of GPUs increases while the total problem size remains fixed. Ideally, doubling the number of GPUs should halve the execution time. For this test, the total filter capacity was fixed at $2^{30}$ slots (approx. 1 billion items), distributed across 2, 4, 6, or 8 GPUs.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=1.0\textwidth]{./images/multi-scaling/strong.pdf}
  \caption{Strong Scaling on System D with a fixed capacity. The Y-axis represents normalized execution time (lower is better), relative to the 2-GPU baseline.}
  \label{fig:multigpu-strong}
\end{figure}

The results in Figure \ref{fig:multigpu-strong} reveal distinct behaviours for different operations:
\begin{itemize}
  \item \textbf{Query and Delete}: These operations show decent scaling. Moving from 2 to 8 GPUs reduces the execution time by approximately 50\%. While this is a noticeable speedup, it is not linear (ideal scaling would yield a 75\% reduction). This indicates that the latency of the All-to-All exchange via NVLink prevents perfect scaling when the work performed by each GPU decreases.
  \item \textbf{Insertion}: Insertion performance is largely flat, showing minimal improvement as GPUs are added. This suggests that for a fixed dataset of this size, the insertion process is dominated by the communication and partitioning phase (hashing keys, calculating destinations, and shuffling data via Gossip) rather than the insertion kernel itself. As more GPUs are added, the complexity of the All-to-All exchange increases, negating the benefit of parallelizing the insertion logic.
\end{itemize}

\FloatBarrier

\subsection{Weak Scaling}
\label{sec:eval:multigpu:weak}

Weak scaling measures how throughput changes as the number of GPUs increases while the workload per GPU remains fixed. Ideally, the normalized throughput should scale linearly with the number of GPUs (e.g., 8 GPUs should provide $4\times$ the throughput of 2 GPUs). For this test, the capacity was fixed at $2^{30}$ slots per GPU, meaning the 8-GPU test processed a massive dataset of $2^{33}$ slots (over 8 billion items).

\begin{figure}[ht!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/multi-scaling/weak.pdf}
  \caption{Weak Scaling on System D with a fixed capacity of $2^{30}$ slots per GPU. The Y-axis represents normalized total throughput relative to the 2-GPU baseline.}
  \label{fig:multigpu-weak}
\end{figure}

Figure \ref{fig:multigpu-weak} illustrates the results:
\begin{itemize}
  \item \textbf{Sub-linear Scaling}: While total throughput increases with more GPUs, it does not scale linearly. At 8 GPUs (processing $4\times$ the data of 2 GPUs), the system achieves roughly $1.95\times$ the query/delete throughput and only $1.2\times$ the insertion throughput.

  \item \textbf{Communication Bottleneck}: This behaviour confirms that even with NVLink, the system is bottlenecked by the interconnect. In a distributed Cuckoo filter, every GPU must communicate with every other GPU. As $N$ increases, the density of the All-to-All communication pattern grows, introducing latency that limits the aggregate throughput.
\end{itemize}

\subsection{Conclusion}

The multi-GPU implementation effectively enables the processing of massive datasets that exceed single-device memory limits (up to 8+ billion items on 8 A100s). However, it is primarily a capacity scaling solution rather than a throughput scaling solution. For datasets that fit within a single GPU, the single-GPU implementation remains far more efficient due to the elimination of communication overhead.