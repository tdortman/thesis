\subsection{Sorted vs. Unsorted Insertion}
\label{sec:eval:sorted-insertion}

To investigate the potential for enhancing insertion throughput by improving memory locality, an alternative insertion strategy was implemented and evaluated. This strategy, detailed in Section \ref{sec:implementation:optimisation-techniques:sorted}, pre-sorts the input keys by their primary bucket index before launching the insertion kernel. The goal is to maximize coalesced memory accesses by ensuring that adjacent threads target the same or nearby buckets.

To strictly isolate the benefits of memory locality from the overhead of the pre-processing step, a \textit{Presorted} metric was also introduced. This metric measures the throughput of the insertion kernel assuming the data has already been sorted "for free", whereas \textit{Sorted} reflects the end-to-end throughput including the radix sort and packing overhead.

\subsubsection{Performance Characteristics}

The performance impact of sorting was evaluated across all three test systems. Figures \ref{fig:sorted:system-a}, \ref{fig:sorted:system-b}, and \ref{fig:sorted:system-c} illustrate the throughput comparison between the standard unsorted approach and the sorted variants across a wide range of input batch sizes.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{images/sorted_insertion/system_a.pdf}
  \caption{Throughput comparison of sorted vs. unsorted insertion on System A (GDDR7)}
  \label{fig:sorted:system-a}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{images/sorted_insertion/system_b.pdf}
  \caption{Throughput comparison of sorted vs. unsorted insertion on System B (HBM3)}
  \label{fig:sorted:system-b}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{images/sorted_insertion/system_c.pdf}
  \caption{Throughput comparison of sorted vs. unsorted insertion on System C (GDDR7)}
  \label{fig:sorted:system-c}
\end{figure}

\FloatBarrier

\subsubsection{Impact on Throughput Curve}
The comparison reveals a significant divergence between the cost of sorting and the benefit of locality:

\begin{itemize}
  \item \textbf{Insertion Efficiency (Presorted)}: The \textit{Presorted} case shows that improved memory locality effectively mitigates the performance penalty usually associated with DRAM-resident filters. While the unsorted throughput collapses as soon as the filter exceeds L2 cache capacity, the presorted insertion maintains a significantly higher and more stable throughput profile.

  \item \textbf{The Sorting Tax}: The substantial gap between the \textit{Presorted} and \textit{Sorted} lines highlights the extreme cost of the pre-processing step. For small, L2-resident inputs, this overhead makes the sorted approach non-viable. However, for large DRAM-resident inputs, the cost of sorting might be justified through the avoidance of random memory access stalls
\end{itemize}

\subsubsection{Architecture-Specific Behaviour}

The speedup provided by pre-sorting depends heavily on the memory technology and bandwidth availability of the GPU.

\begin{itemize}
  \item \textbf{GDDR7 Systems (A \& C)}: On systems relying on GDDR7, the benefits of pre-sorting are massive. In the DRAM-resident scenario ($>2^{25}$ elements), the pure insertion kernel (excluding sort) achieves approximately 3$\times$ higher throughput compared to the unsorted baseline. This confirms that random memory accesses are a big bottleneck on these architectures. However, when the sorting overhead is included, the net throughput only matches or slightly exceeds the unsorted baseline at maximum capacity.

  \item \textbf{HBM3 System (B)}: On System B, the high-bandwidth memory already absorbs much of the random access penalty inherent to the unsorted approach. Consequently, while pre-sorting still yields a performance improvement, the speedup is less dramatic. Because the speedup factor is smaller, it fails to amortize the cost of the sort, resulting in a net loss in total throughput (including sort) compared to the standard unsorted method.
\end{itemize}

\subsubsection{Trade-offs and Limitations}

The breakdown of sorted vs. unsorted performance leads to some conclusions regarding viability:

\begin{itemize}
  \item \textbf{Overhead vs. Gain}: While memory locality theoretically solves the random-access bottleneck, the current cost of the sorting and packing step is too high to realize these gains in an end-to-end pipeline. Unless the sorting cost can be amortized over other operations, the standard unsorted insertion remains superior for general use.

  \item \textbf{Memory Overhead}: The sorting process requires auxiliary buffers, doubling the peak memory usage during insertion. This effectively halves the maximum batch size.
\end{itemize}