\section{Cache Efficiency}
\label{sec:eval:cache}

To understand how each filter interacts with the GPU's memory hierarchy, the L1 and L2 cache hit rates were measured by profiling the relevant kernels using NVIDIA Nsight Compute (ncu) on System B. While cache hit rate is not a direct proxy for overall throughput, it provides crucial insight into the memory access patterns and architectural bottlenecks of each implementation. The results for Insert, Query, and Delete operations are presented in Figure \ref{fig:cache-hit-rates}.

\subsection{L1 Cache Analysis}
A consistent trend across all operations is the exceptionally high L1 hit rate (near 100\%) for both the TCF and the GQF, though they achieve this through different mechanisms.

The TCF relies heavily on shared memory for internal computations. It utilizes cooperative groups to load blocks into shared memory and perform intra-block sorting and coordination. Since shared memory physically occupies the same space as the L1 cache on modern NVIDIA architectures, these accesses are recorded as L1 hits.

In contrast, the GQF does not explicitly utilize shared memory but achieves high L1 efficiency through extreme spatial locality. The implementation assigns individual threads to manage specific, contiguous regions of the filter. As a thread performs operations such as linear probing or shifting elements within a run, it repeatedly accesses the same range of memory addresses. This ensures that the relevant cache lines remain resident in L1, resulting in a near-perfect hit rate despite operating directly on global memory structures.

The Cuckoo filter exhibits a moderate L1 hit rate (typically between 30\% and 60\%). This reflects its reliance on atomic operations on global memory addresses. While some metadata or repeated accesses may hit in L1, the random nature of hashing means that successive memory requests from a warp rarely map to the same L1 cache line, preventing the high hit rates seen in the locality-optimized implementations.

\subsection{L2 Cache Analysis}

The L2 cache hit rates provide a clear visualization of the transition from a cache-resident workload to a DRAM-resident workload. For the Cuckoo filter and the Blocked Bloom filter, the L2 hit rate remains high (approximately 80-90\%) for smaller capacities. However, a sharp decline is observed once the filter size exceeds $2^{24}$ elements. This inflection point roughly corresponds to the physical L2 cache size of the test GPU. The steep drop-off confirms that beyond this point, every operation effectively incurs a global memory access, explaining the shift in performance scaling discussed in Section \ref{sec:eval:throughput}.

The GQF exhibits similar behavior for its Lookup operation. Because a GQF lookup involves linearly scanning a cluster of slots in memory, it relies heavily on the L2 cache to minimize latency. Once the filter grows too large, these linear scans result in frequent cache misses, aligning its curve with that of the Cuckoo and Bloom filters.

In contrast, the TCF and GQF (specifically for Insertion and Deletion) maintain a consistently high L2 hit rate across all filter sizes. This unusual stability indicates that these algorithms interact with global memory far less frequently than the Cuckoo or Bloom filters. Instead, they perform the vast majority of their work, such as sorting items within a block or managing cooperative groups, using internal registers and shared memory. While this results in high cache statistics, it confirms the architectural analysis from the previous section: these filters are bound by the speed of the GPU's compute units and shared memory (SRAM), preventing them from utilizing the abundant DRAM bandwidth available on modern High-Bandwidth Memory systems.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/cache-hit-rates.pdf}
  \caption{L1 and L2 cache hit rates for the various operations across varying filter capacities.}
  \label{fig:cache-hit-rates}
\end{figure}

\FloatBarrier
