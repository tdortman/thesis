\section{Probabilistic Data Structures}
\label{sec:background:probabilistic-data-structures}

At the heart of this thesis is the need for a dynamic and efficient data structure for approximate set membership. This section reviews the key structures that form the basis of the work presented. The analysis begins with the classic Bloom filter and its locality-optimized variant, which serve as important performance baselines. Following this is an explanation of the mechanics of Cuckoo hashing, the eviction-based strategy that enables the dynamic properties of the target data structure. Finally, the Cuckoo filter is detailed, showing how it combines these concepts to create a powerful and flexible alternative.

\subsection{Bloom Filter}
\label{sec:background:bloom-filter}

Invented in 1970, the Bloom filter has long been the dominant probabilistic data structure for approximate membership query (AMQ) problems. Its operation is based on a simple yet effective concept: a bit array of size $m$ and a set of $k$ independent hash functions. To insert an item, the item is hashed $k$ times, and each resulting hash value is used as an index to set a bit in the array to 1. To query for an item's membership, the item is again hashed $k$ times. If all corresponding bits in the array are 1, the item is considered to possibly be in the set. However, if even one bit is 0, the item is definitively not in the set, as illustrated in Figure \ref{fig:bloom-filter}.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7\textwidth]{images/bloom-filter.png}
  \caption{An illustration of a Bloom filter's insertion and lookup mechanism with $m=11$ slots and $k=3$ hash functions. The set $\{x, y, z\}$ has been inserted, setting the corresponding bits in the array to 1. A membership query for a new item $w$ returns a definitive "no" (guaranteeing no false negatives) because one of the bits it maps to is 0.}
  \label{fig:bloom-filter}
\end{figure}

The false-positive rate $\epsilon$ of a Bloom filter after inserting $n$ items is approximately:
\begin{equation}
  \epsilon \approx \left(1 - e^{-nk/m}\right)^k
  \label{eq:bloom-fpr}
\end{equation}

This rate is optimized by choosing the optimal number of hash functions, which for a given $n$ and $m$ is:
\begin{equation}
  k = \frac{m}{n} \ln 2
\end{equation}

Given a target false positive rate $\epsilon$, the number of hash functions used by a space-optimized Bloom filter is given by:

\begin{equation}
  k = \log_2(1/\epsilon)
\end{equation}

Such a filter uses $1.44 \log_2(1/\epsilon)$ bits per element.

Despite its widespread use, the classic Bloom filter has several notable disadvantages:

\begin{itemize}
  \item \textbf{No Deletion}: The standard implementation does not support the removal of items, as clearing a bit could inadvertently remove other items that hash to the same location. Variants like the Counting Bloom Filter \cite{counting-bloom} address this by using counters instead of single bits, but at a significant cost to space efficiency.
  \item \textbf{Linear Complexity}: Lookup and insertion performance is dependent on the number of hash functions, which scales linearly with $m$. This can lead to performance bottlenecks in high-throughput scenarios.
  \item \textbf{Poor Memory Locality}: The $k$ hash functions produce indices that are typically scattered randomly across the entire bit array. This leads to poor cache performance on CPUs and is particularly detrimental on GPUs, where it prevents efficient memory access.
  \item \textbf{Degrading Performance}: As the filter fills up and more bits are set to 1, the false positive rate steadily increases, eventually converging to a point where all queries yield a positive result.
  \item \textbf{Suboptimal Space Usage}: The optimal space usage for a Bloom filter is approximately 44\% higher than the information theoretical lower bound for AMQ data structures. Many modern filters, including the Cuckoo filter, achieve a lower overhead relative to this bound.
\end{itemize}

\subsection{Blocked Bloom Filter}
\label{sec:background:blocked-bloom-filter}

To address the issue of poor memory locality, the Blocked Bloom Filter was introduced. As this variant is a primary point of comparison in this thesis, its design warrants a more detailed explanation.

Instead of a single monolithic bit array, the Blocked Bloom Filter partitions the array into an array of smaller, independent Bloom filters, called blocks. Each block is typically sized to fit within a single CPU cache line (e.g., 64 bytes). The hashing scheme is modified: a single hash function is first used to map an incoming item to a specific block. Then, the standard $k$ hash functions are used to set or check bits only within that selected block, as shown in Figure \ref{fig:blocked-bloom-filter}.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7\textwidth]{images/blocked-bloom-filter.png}
  \caption{An illustration of a Blocked Bloom Filter. Each item is first mapped to a single block (e.g., item $y$ maps to Block 0, while $x$ and $z$ map to Block 1). The $k$ hash functions then operate only within that selected block, improving memory locality. A query for item $w$, which maps to Block 0, returns a definitive "no" as one of its target bits is 0.}
  \label{fig:blocked-bloom-filter}
\end{figure}

The primary advantage of this design is a dramatic improvement in memory locality. All $k$ memory accesses for a single item are now confined to a small, contiguous memory region. This is highly beneficial for modern CPU caches and, more importantly for this thesis, is extremely well-suited to the parallel memory access patterns of GPUs. When multiple threads in a warp process items that map to the same block, their memory requests can be coalesced into a single transaction, significantly improving memory bandwidth utilization.

However, this performance gain comes at the cost of a higher false positive rate. By partitioning the filter, the Blocked Bloom Filter loses the "averaging" effect of the classic design. If an unlucky distribution of items causes one block to become heavily saturated, its local false positive rate will increase dramatically, and this "hotspot" can dominate the overall false positive rate of the entire structure. Therefore, the Blocked Bloom Filter represents a direct trade-off: sacrificing some statistical efficiency for significantly better performance, making it a highly relevant baseline for evaluating cache-aware and GPU-accelerated data structures.

\subsection{Cuckoo Hashing}
\label{sec:background:cuckoo-hashing}

Cuckoo hashing, introduced by Pagh and Rodler in 2004 \cite{pagh2004cuckoo}, is a powerful hashing scheme that provides a significant advantage over many others: a worst-case $O(1)$ lookup time \cite{pagh2004cuckoo}. Its name is derived from the cuckoo bird, which is known for laying its eggs in the nests of other birds, often forcing the original occupants out. The scheme's elegance lies in its conceptual simplicity compared to previous methods that also offered worst-case constant lookup guarantees.

In its original formulation, the scheme uses two independent hash tables, $T_1$ and $T_2$, each of size $r$, and two independent hash functions $h_1$ and $h_2$. The core invariant of Cuckoo hashing is that for any given item $x$, it is stored in one of two possible locations: either $\mathrm{T}_1[\mathrm{h}_1(x) \bmod r]$ or $\mathrm{T}_2[\mathrm{h}_2(x) \bmod r]$. A lookup operation is therefore guaranteed to be $O(1)$, as it only requires checking these two specific locations.

The insertion process is more involved and follows the distinct "cuckoo" eviction protocol, as illustrated in Figure \ref{fig:cuckoo-hashing}. The steps are as follows:

\begin{enumerate}
  \item For a new item $x$, its two potential locations $i_1 = h_1(x) \bmod r$ and \\ $i_2 = h_2(x) \bmod r$ are computed.

  \item If either $\mathrm{T}_1[\mathrm{i}_1]$ or $\mathrm{T}_2[\mathrm{i}_2]$ is empty, the item is placed there, and the insertion is complete.

  \item If both slots are occupied, an existing item (say $y$) is evicted from one of the location (e.g. $i_1$) and $x$ is placed there.

  \item The evicted item $y$ is then reinserted into its alternate location. If $y$ was in $T_1$, it now attempts to move to $\mathrm{T}_2[\mathrm{i}_2]$. This move may, in turn, cause another eviction, leading to a chain of evictions.

  \item To prevent infinite loops, a maximum number of evictions is set. If this limit is reached, the table is considered full, and all items must be rehashed with a new pair of hash functions, potentially into larger tables.
\end{enumerate}

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7\textwidth]{images/cuckoo-hashing.png}
  \caption{Illustration of a Cuckoo hashing insertion in a single-table variant. The incoming item $x$ finds both of its candidate slots occupied. It evicts item $a$, triggering a cascading eviction chain until item $d$ is moved to a free slot, resulting in a new stable arrangement of items in the table.}
  \label{fig:cuckoo-hashing}
\end{figure}

The success of this probabilistic scheme hinges on the load factor, the theory guarantees that if the tables are kept sufficiently sparse. Specifically, if $r$ is greater than $(1+\epsilon)n$, where $n$ is the number of items, the probability of an insertion failing and requiring a rehash is very low \cite{pagh2001cell}. This effectively means the total load factor across both tables should be kept below 50\%.

A common and important variant is to use a single table of size $m = 2r$. This is more space-efficient and is the model that more closely resembles a Cuckoo filter's implementation. It also introduces the idea of deriving an alternate location from a current one without storing any extra state.

Pagh and Rodler describe a trick, attributed to John Tromp, that achieves this. The key insight is to redefine the two potential locations for an item $x$. Instead of simply being $h_1(x) \bmod m$ and $h_2(x) \bmod m$, the locations are defined as:

\begin{itemize}
  \item $i_1 = h_1(x) \bmod m$
  \item $i_2 = (h_2(x) - h_1(x)) \bmod m$
\end{itemize}

With this specific construction, a symmetric function can be used to jump between the two locations. If an item $x$ is currently at a location $i$, its alternate location $i'$ can be calculated using the mapping

\begin{equation}
  i' = (h_2(x) - i) \bmod m
\end{equation}

This works because the transformation is its own inverse for that specific pair of locations. Applying it to $i_1$ yields $i_2$, and applying it again to $i_2$ yields $i_1$. This concept of a state-free, symmetric, and computable mapping is the direct precursor to the partial-key hashing scheme used in Cuckoo filters.

\subsection{Cuckoo Filter}
\label{sec:background:cuckoo-filter}

The Cuckoo filter, introduced by Fan et al., is a probabilistic data structure that adapts the principles of Cuckoo hashing to provide a highly space-efficient and performant alternative to the Bloom filter, most notably by supporting deletion of items \cite{og-cuckoo-filter}. It improves upon standard Cuckoo hashing with several important distinctions.

First, instead of storing entire items, a Cuckoo filter only stores a small fingerprint for each item, which is a fixed-size sequence of bits derived from the item's hash. This significantly reduces the memory footprint. A direct consequence, however, is that traditional rehashing is not an option, as the original items are not available to be re-inserted. Therefore, the filter's size and parameters are typically static once it is created.

Second, it replaces the standard Cuckoo hashing scheme with partial-key cuckoo hashing. This clever technique establishes a symmetric dependency between the two potential bucket locations and the item's fingerprint. A single hash function is used to compute an initial location $i_1$ and a fingerprint $fp$. The alternate location $i_2$ is then derived using the XOR operation:

\begin{equation}
  i_2 = i_1 \oplus \mathrm{hash}(fp)
  \label{eq:partial-cuckoo-hashing}
\end{equation}

Thanks to the symmetric nature of XOR, this relationship works both ways, such that $i_1 = i_2 \oplus \mathrm{hash}(fp)$. This allows an evicted fingerprint to calculate its alternate location from its current position and its own fingerprint alone, a crucial feature since the original item is not stored.

\subsubsection{Design and Performance Characteristics}
\label{sec:background:cuckoo-filter:design-perf}

The design of the Cuckoo filter leads to several practical advantages over Bloom filters and their variants.

\begin{itemize}
  \item \textbf{High Space Utilization}: By allowing items to be relocated during insertion, Cuckoo filters can achieve very high load factors. The paper demonstrates that with a bucket size of 4, the filter can consistently reach an occupancy of over 95\%. The GPU implementation in this thesis generally prefers larger bucket sizes for performance reasons, thus consistently reaching a load factor of over 99\%.

  \item \textbf{Space Efficiency vs. False Positive Rate}: The false positive rate $\epsilon$ of a Cuckoo filter is directly tied to the fingerprint size $f$ and the bucket size $b$. Because a lookup must check up to $2b$ fingerprints in the worst case, the false positive rate can be approximated by:

    \begin{equation}
      \epsilon \approx 2b/2^f
    \end{equation}

    The actual amortized space cost per item, $C$, is the fingerprint size $f$ divided by the filter's load factor $\alpha$, since the cost of the unoccupied slots must be distributed among the stored items. The minimal fingerprint size required to achieve a target rate $\epsilon$ is approximately

    \begin{equation}
      f \ge \log_2(2b/\epsilon) = \log_2(1/\epsilon) + \log_2(2b)
    \end{equation}

    By substituting this minimal required value for $f$ into the cost definition, an upper bound for the amortized space cost can be established:

    \begin{equation}
      C \le \frac{\log_2(1/\epsilon) + \log_2(2b)}{\alpha}
      \label{eq:cuckoo-space-bound}
    \end{equation}

    This equation is important as it decomposes the cost into the information-theoretic lower bound ($\log_2(1/\epsilon)$), an overhead term related to the bucket size ($\log_2(2b)$), and an efficiency penalty from the load factor ($1/\alpha$). It clearly reveals the central trade-off: a larger bucket size $b$ improves the load factor $\alpha$ but also increases the overhead term, requiring careful tuning to optimize space. For a target $\epsilon < 3\%$, the authors show that a well-configured Cuckoo filter is more space-efficient than a space-optimized Bloom filter \cite{og-cuckoo-filter}.

  \item \textbf{Performance}: A key performance advantage is that any lookup, positive or negative, always reads a fixed number of buckets, resulting in (at most) two cache line misses. This is a significant improvement over standard Bloom filters, where the number of memory probes $k$ scales with the desired false positive rate and can be much larger than two. This theoretical efficiency translates to strong practical performance. As demonstrated later in the evaluation in Section \ref{sec:eval:throughput}, while it does not fully match the raw throughput of the highly cache-optimized (and non-deletable) Blocked Bloom Filter, a well-managed GPU Cuckoo Filter is highly competitive and easily surpasses all other tested filters.
\end{itemize}
