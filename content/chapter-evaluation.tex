\chapter{Evaluation}
\label{sec:evaluation}

The performance of the GPU-accelerated Cuckoo filter is evaluated to demonstrate its throughput, scalability, and resource efficiency. This chapter details the experimental setup, the baselines used for comparison, and the analysis of the results across various metrics.

\section{Experimental Setup}
\label{sec:eval:setup}

All performance tests use 16-bit fingerprints (or equivalent space allocation for the Blocked Bloom filter) and random 64-bit integers as input keys. The experiments evaluate both positive lookups (items present in the filter) and negative lookups (items absent from the filter), as well as insertion throughput.

\subsection{Reference Implementations}
To provide a comprehensive analysis, the proposed GPU Cuckoo filter is compared against the following data structures:

\begin{itemize}
  \item \textbf{GPU Blocked Bloom Filter}: Sourced from the \textit{cuCollections} library, serving as a high-performance baseline for a static (non-deletable) filter. \cite{cuCollections}

  \item \textbf{Original CPU Cuckoo Filter}: The reference implementation from the original 2014 paper by Fan et al. \cite{og-cuckoo-filter}.

  \item \textbf{Optimized Partitioned CPU Cuckoo Filter}: A custom implementation utilizing multithreading (one thread per partition) to maximize CPU throughput without the overhead of SIMD instructions, which were found to degrade performance in this specific context. \cite{partitioned-cuckoo}

  \item \textbf{GPU Quotient Filter}: Included for a basic comparison. Note that its restrictive parameterization makes it largely unsuitable for the massive batch use-cases targeted by this thesis. \cite{gpu-qf}

  \item \textbf{Bulk Two-Choice Filter (TCF)}: A modern, GPU-focused AMQ data structure that serves as a direct competitor. \cite{tcf}
\end{itemize}

\subsection{Evaluation Metrics}
The implementations are assessed using the following metrics:

\begin{itemize}
  \item \textbf{Throughput}: Measured in millions of operations per second for insertions and lookups. This is evaluated under two distinct conditions:
    \begin{itemize}
      \item \textbf{L2-Resident}: A filter size small enough to fit entirely within the GPU's L2 cache.
      \item \textbf{DRAM-Resident}: A larger filter size that necessitates global memory access.
    \end{itemize}

  \item \textbf{Bucket Size Impact}: An analysis of how different fixed bucket sizes affect the performance of the GPU Cuckoo filter (Section \ref{sec:eval:bucket-size}).

  \item \textbf{False Positive Rate (FPR)}: The empirical FPR is measured for given load factors and filter sizes to verify adherence to theoretical bounds (Section \ref{sec:eval:fpr}).

  \item \textbf{Cache Efficiency}: L1 and L2 cache hit rates are measured to evaluate memory locality (Section \ref{sec:eval:cache}).

  \item \textbf{Hardware Utilization}: Resource usage is analysed as a percentage of the theoretical peak throughput ("Speed of Light") for compute, memory bandwidth, and cache bandwidth (Section \ref{sec:eval:sol}).
\end{itemize}

\section{Throughput Analysis}
\label{sec:eval:throughput}

\subsection{L2-Resident Filters}
\label{sec:eval:throughput:l2-resident}

\subsection{DRAM-Resident Filters}
\label{sec:eval:throughput:dram-resident}

\section{Bucket Size Impact}
\label{sec:eval:bucket-size}

The \texttt{bucketSize} parameter, defining the number of fingerprints stored in each bucket, plays an important role in the filter's overall performance. Testing demonstrates that performance degrades at both the lower and upper extremes of bucket sizing, necessitating a careful balance between algorithmic overhead and memory access granularity.

\subsection{Performance Trade-offs}

\begin{itemize}
  \item \textbf{Small Buckets}: Configuring the filter with very small buckets negatively impacts performance. With fewer slots per bucket, the probability of a collision increases, as does the likelihood that a bucket is full. This increases the average number of buckets that must be accessed and loaded from memory to complete an insertion or a lookup, resulting in higher latency.

  \item \textbf{Large Buckets}: Excessively large buckets introduce hardware-level inefficiencies. If a bucket's size exceeds the GPU's cache line size (typically 128 bytes), fetching a single logical bucket requires multiple physical memory transactions. In the worst case, this doubles the number of cache lines that must be fetched, effectively halving the effective memory bandwidth.
\end{itemize}

\subsection{Optimal Configuration}

The experiments in Figure \ref{fig:bucket-size} identify distinct optimal configurations for insertion and lookup operations depending on the working set size:

\begin{itemize}
  \item \textbf{Insertion}: A bucket size of 16 fingerprints was found to be the fastest configuration in all tested scenarios. This size appears to offer the optimal trade-off, providing enough slots to minimize eviction chains without incurring the bandwidth penalty of multi-cache-line fetches.

  \item \textbf{Lookup (L2-Resident)}: When the filter is sized to fit entirely within the GPU's L2 cache, a bucket size of 8 fingerprints yields the highest throughput. This is driven by instruction-level efficiency. A bucket of this size can be represented as two 64-bit words or a single 128-bit vector. This allows the entire bucket to be loaded into registers via a single vectorized machine instruction, maximizing the throughput of the L1/L2 cache hierarchy.

  \item \textbf{Lookup (DRAM-Resident)}: Once the filter size exceeds the L2 cache capacity, the bottleneck shifts to global memory bandwidth. In this case, a bucket size of 16 becomes favourable again. Since fetching data from DRAM creates a significant latency penalty, it is more efficient to process a larger "middle ground" bucket size that maximizes the utility of the data fetched in a standard 128-byte DRAM burst.
\end{itemize}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{images/bucket-size.png}
  \caption{Normalized throughput of the GPU Cuckoo filter for different bucket sizes.}
  \label{fig:bucket-size}
\end{figure}

\FloatBarrier
\section{False Positive Rate (FPR)}
\label{sec:eval:fpr}

To evaluate the reliability of the implemented filters, the empirical false-positive rate was measured across a range of filter capacities. For each test, the filters were populated to a constant 95\% load factor using random keys. The total memory size was varied from $2^{15}$ to $2^{30}$ bytes, allowing each implementation to optimize its internal layout within that fixed memory constraint. The results are presented in \ref{fig:fpr-vs-memory}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/fpr.png}
  \caption{Comparison of False Positive Rates (FPR) versus total memory size for various filter implementations at a 95\% load factor.}
  \label{fig:fpr-vs-memory}
\end{figure}

\subsection{Analysis}

\begin{itemize}
  \item \textbf{Blocked Bloom Filter}: The Blocked Bloom filter demonstrates the highest false-positive rate among all tested structures. This is a known characteristic of the blocked design, partitioning the bit array into small, fixed-size blocks prevents the "averaging" of hash collisions across the entire filter. As a result, a few heavily congested blocks can disproportionately skew the overall error rate. Notably, it is the only filter where the false-positive rate actively degrades as the total memory size increases, indicating poor scalability in terms of accuracy for massive datasets.

  \item \textbf{CPU vs. GPU Cuckoo Filters}: A clear distinction is visible between the CPU and GPU Cuckoo filter implementations. The CPU versions (both standard and partitioned) achieve the lowest false-positive rates, hovering near 0.005\% and 0.01\% respectively. The GPU Cuckoo filter, while still highly accurate, exhibits a higher rate of approximately 0.045\%. This difference is a direct consequence of the bucket size trade-off discussed in Section \ref{sec:eval:bucket-size}. To maximize parallel throughput, the GPU implementation uses a bucket size of 16, whereas the CPU versions use a standard bucket size of 4. As established in Equation \ref{eq:cuckoo-space-bound}, a larger bucket size linearly increases the collision probability for a fixed fingerprint size.

  \item \textbf{Comparison with Other GPU Filters}: Despite the trade-off required for throughput, the GPU Cuckoo filter maintains the lowest false-positive rate among the GPU-accelerated candidates. It significantly outperforms the Bulk Two-Choice Filter (TCF), which exhibits an error rate an order of magnitude higher (around 0.7\% - 1.1\%). This confirms that the Cuckoo filter's design offers a superior balance of speed and accuracy for high-performance GPU applications.
\end{itemize}

\section{Cache Efficiency}
\label{sec:eval:cache}

\section{Hardware Utilization}
\label{sec:eval:sol}
