\chapter{Evaluation}
\label{sec:evaluation}

The performance of the GPU-accelerated Cuckoo filter is evaluated to demonstrate its throughput, scalability, and resource efficiency. This chapter details the experimental setup, the baselines used for comparison, and the analysis of the results across various metrics.

\section{Experimental Setup}
\label{sec:eval:setup}

The performance evaluation was conducted on two distinct hardware configurations to analyse how different memory architectures impact the scalability of the filters. Systems C is only used for the comparison between sorted and unsorted insertion as well as the comparison of the two eviction policies.

\begin{itemize}
  \item \textbf{System A (GDDR7)}: An AMD EPYC 7713P (64 cores) paired with an NVIDIA RTX PRO 6000 Blackwell GPU featuring 96 GB of GDDR7 memory (1.8 TB/s). The system runs AlmaLinux 10.1 with NVIDIA driver 580.95.05 and CUDA 12.9.86.
  \item \textbf{System B (HBM3)}: An NVIDIA GH200 Grace Hopper system with 72 ARM Neoverse V2 cores and an H100 GPU with 96 GB of HBM3 memory (3.4 TB/s). The system runs Ubuntu 24.04.3 with NVIDIA driver 580.105.08 and CUDA 13.0.88.
\end{itemize}

It is important to note that these systems are not compared head-to-head to determine which is "better." Rather, they are used to evaluate how the algorithms respond to high-bandwidth memory (HBM) versus standard graphics memory (GDDR).

All performance tests use 16-bit fingerprints (with equivalent space allocation for the Blocked Bloom filter) and random 64-bit integers as input keys.

\subsection{Reference Implementations}
\label{sec:eval:baselines}
To provide a comprehensive analysis, the proposed GPU Cuckoo filter is compared against the following data structures:

\begin{itemize}
  \item \textbf{GPU Blocked Bloom Filter}: Sourced from the \textit{cuCollections} library, serving as a high-performance baseline for a static (non-deletable) filter. \cite{cuCollections}

  \item \textbf{Original CPU Cuckoo Filter}: The reference implementation from the original 2014 paper by Fan et al. \cite{og-cuckoo-filter}.

  \item \textbf{Optimized Partitioned CPU Cuckoo Filter}: A custom implementation utilizing multithreading (one thread per partition) to maximize CPU throughput without the overhead of SIMD instructions, which were found to degrade performance in this specific context. This filter is not tested for System B, as it relies on x86-64 SIMD instructions. \cite{partitioned-cuckoo}

  \item \textbf{Bulk Two-Choice Filter (TCF)}: A modern, GPU-focused AMQ data structure that serves as a direct competitor. \cite{tcf}

  \item \textbf{GPU Counting Quotient Filter (GQF)}: A highly space-efficient probabilistic data structure sourced from the same study as the TCF. This implementation utilizes efficient rank-and-select primitives to minimize metadata overhead and supports counting, resizing, and deletions. \cite{tcf}
\end{itemize}

\subsection{Evaluation Metrics}
\label{sec:eval:metrics}
The implementations are assessed using the following metrics:

\begin{itemize}
  \item \textbf{Throughput}: Measured for insertions, lookups, and deletions and is evaluated under two distinct conditions:
    \begin{itemize}
      \item \textbf{L2-Resident}: A filter size small enough to fit entirely within the GPU's L2 cache.
      \item \textbf{DRAM-Resident}: A larger filter size that necessitates global memory access.
    \end{itemize}

  \item \textbf{False Positive Rate}: The empirical FPR is measured for given load factors and filter sizes to verify adherence to theoretical bounds (Section \ref{sec:eval:fpr}).

  \item \textbf{Cache Efficiency}: L1 and L2 cache hit rates are measured to evaluate memory locality (Section \ref{sec:eval:cache}).

  \item \textbf{Hardware Utilization}: Resource usage is analysed as a percentage of the theoretical peak throughput ("Speed of Light") for compute, DRAM and cache bandwidth (Section \ref{sec:eval:sol}).
\end{itemize}

\section{Throughput Analysis}
\label{sec:eval:throughput}

This section analyses the raw throughput of the filters in millions of operations per second (MOPS). The results highlight the GPU Cuckoo filter's ability to compete with the static Blocked Bloom filter while significantly outperforming other dynamic alternatives.

\subsection{L2-Resident Filters}
\label{sec:eval:throughput:l2-resident}

In this scenario, the filter size (approx. 4.2 million items) is small enough to fit entirely within the GPU's L2 cache. This isolates the algorithmic efficiency and instruction throughput from main memory latency.

\subsubsection{Insertion Performance}
Figures \ref{fig:insert-mon02-small} and \ref{fig:insert-gh200-small} illustrate the insertion throughput. The results demonstrate that the GPU Cuckoo filter is highly competitive, bridging the gap between static and dynamic data structures.

While the static Blocked Bloom filter maintains a performance lead of approximately 2x due to its much simpler, append-only memory access pattern, the Cuckoo filter establishes itself as the clear performance leader among the dynamic data structures. It achieves throughput significantly higher than the TCF and orders of magnitude higher than the CPU versions. This shows that the proposed parallel eviction strategy successfully minimizes the overhead typically associated with supporting deletions.

In contrast, the GQF's insertion performance is rather poor with small batches, consistently lagging behind even the CPU-based implementations. This indicates that the GQF's locking overhead and metadata management create a significant bottleneck that prevents it from saturating the GPU's resources for smaller workloads.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/perf/insert_perf_mon02_small.png}
  \caption{Insert Performance on System A (GDDR7) for an L2-resident filter}
  \label{fig:insert-mon02-small}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/perf/insert_perf_gh200_small.png}
  \caption{Insert Performance on System B (HBM3) for an L2-resident filter}
  \label{fig:insert-gh200-small}
\end{figure}

\FloatBarrier

\subsubsection{Lookup Performance}
The query performance is evaluated by measuring throughput for both positive lookups (keys present in the filter) and negative lookups (keys absent). The results for the GDDR7 system are presented in Figure \ref{fig:query-mon02-small}, while the HBM3 system results are shown in Figure \ref{fig:query-gh200-small}.

For positive lookups, the GPU Cuckoo filter demonstrates exceptional efficiency, nearly matching the Blocked Bloom filter on GDDR7 and outperforming it on HBM3. This performance is driven by the implementation's bias towards placing items in their primary bucket. With the larger bucket size of 16, there is a high probability that an existing item resides in its primary location. Consequently, the lookup algorithm frequently short-circuits after loading only the first bucket, requiring just a single memory transaction (or cache hit).

In the case of negative lookups, the throughput is roughly halved. To definitively rule out the presence of an item, the algorithm must inspect both candidate buckets. This requires two distinct memory accesses, doubling the bandwidth requirement compared to a successful primary-bucket hit. This is different from the TCF, which utilizes cooperative groups to always load both candidate blocks in parallel to minimize thread divergence. As a result, it exhibits symmetric performance for positive and negative queries, whereas the Cuckoo filter optimizes specifically for the positive case.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/perf/query_perf_mon02_small.png}
  \caption{Query Performance on System A (GDDR7) for an L2-resident filter.}
  \label{fig:query-mon02-small}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/perf/query_perf_gh200_small.png}
  \caption{Query Performance on System B (HBM3) for an L2-resident filter.}
  \label{fig:query-gh200-small}
\end{figure}

\FloatBarrier

\subsubsection{Deletion Performance}
Deletion performance, illustrated in Figure \ref{fig:delete-mon02-small} and \ref{fig:delete-gh200-small}, highlights the most significant performance advantage of the GPU Cuckoo filter over its competitors.

As the logarithmic scale demonstrates, the Cuckoo filter is orders of magnitude faster than both the TCF and the GQF. This performance gap comes from the fundamental algorithmic differences in how deletions are handled:
\begin{itemize}
  \item \textbf{Cuckoo Filter}: Deletion is a localized, atomic compare-and-swap (CAS) operation. Once the target fingerprint is located, a single atomic instruction is sufficient to remove it.
  \item \textbf{GQF \& TCF}: These filters require complex metadata updates or element shifting to maintain their structural invariants (e.g., maintaining sorted runs or tombstones). These operations introduce significant overhead, severely limiting throughput.
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/perf/delete_perf_mon02_small.png}
  \caption{Deletion Performance on System A (GDDR7) for an L2-resident filter.}
  \label{fig:delete-mon02-small}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/perf/delete_perf_gh200_small.png}
  \caption{Deletion Performance on System B (HBM3) for an L2-resident filter.}
  \label{fig:delete-gh200-small}
\end{figure}

\FloatBarrier

\subsection{DRAM-Resident Filters}
\label{sec:eval:throughput:dram-resident}

For larger datasets (here tested with approx. 268 million items), the filter size exceeds the GPU's L2 cache capacity and thus performance is primarily limited by the memory subsystem. The comparison between System A (GDDR7) and System B (HBM3) reveals an important difference in how the different data structures scale with modern hardware architectures.

\subsubsection{Impact of Hardware Architecture}
\label{sec:eval:throughput:dram-scaling}

A consistent trend observed across all operations (Insertion, Lookup, and Deletion) reveals a fundamental difference in how the different data structures utilize modern hardware. The filters fall into two distinct scaling categories:

\begin{itemize}
  \item \textbf{DRAM-Bound Filters (Cuckoo \& Blocked Bloom)}: Both the GPU Cuckoo filter and the Blocked Bloom filter demonstrate strong scalability with global memory bandwidth. Their throughput increases significantly when moving from GDDR7 to HBM3, even though the HBM3 system has 30\% fewer CUDA cores. This confirms that their performance is primarily limited by how fast data can be moved from DRAM.

  \item \textbf{SRAM-Bound Filters (TCF \& GQF)}: In contrast, the TCF and GQF show stagnant or even regressive performance on the HBM3 system. This indicates that these filters are bound not by global DRAM bandwidth, but by the speed of Shared Memory (SRAM). As detailed in the cache efficiency analysis (Section \ref{sec:eval:cache}), these filters rely heavily on complex intra-warp coordination, cooperative groups, and local sorting within shared memory. This creates a scalability bottleneck: because DRAM bandwidth is currently growing significantly faster than SRAM speed in GPU architectures, filters that rely on heavy local computation per byte of data fetched can face a "scalability wall".
\end{itemize}

\subsubsection{Insertion Performance}

The insertion throughput for the DRAM-resident workload is illustrated in Figure \ref{fig:insert-mon02-large} for the GDDR7 system and Figure \ref{fig:insert-gh200-large} for the HBM3 system.

Consistent with the architectural analysis above, the Cuckoo filter maintains high throughput in the DRAM-resident case. On the GDDR7 system, it competes closely with the Blocked Bloom filter. On the HBM3 system, the Blocked Bloom filter pulls ahead slightly because its linear writes are more efficient than the random atomic transactions required by the Cuckoo filter. However, the Cuckoo filter remains the fastest dynamic option by a wide margin, as the GQF and TCF struggle to hide the latency of global memory accesses due to their complex internal logic.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/perf/insert_perf_mon02_large.png}
  \caption{Insertion Performance on System A (GDDR7) for a DRAM-resident filter.}
  \label{fig:insert-mon02-large}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/perf/insert_perf_gh200_large.png}
  \caption{Insertion Performance on System B (HBM3) for a DRAM-resident filter.}
  \label{fig:insert-gh200-large}
\end{figure}

\FloatBarrier

\subsubsection{Lookup Performance}
Query performance is presented in Figure \ref{fig:query-mon02-large} for the GDDR7 system and Figure \ref{fig:query-gh200-large} for the HBM3 system.

Across both systems, the Cuckoo filter maintains a distinct performance profile based on the query result:
\begin{itemize}
  \item \textbf{Positive Lookups}: Positive queries (solid blue line) consistently achieve higher throughput than negative queries. The larger bucket size $(b=16)$ ensures that most items reside in their primary bucket, allowing the filter to satisfy the majority of positive requests with a single DRAM transaction.

  \item \textbf{Negative Lookups}: Throughput for negative lookups (dashed blue line) is about half that of positive lookups. To definitively confirm an item is absent, the algorithm must check both candidate buckets, doubling the memory bandwidth requirement per query. This contrasts with the TCF, where positive and negative query performance is symmetric because the cooperative groups always load both blocks in parallel.
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/perf/query_perf_mon02_large.png}
  \caption{Query Performance on System A (GDDR7) for a DRAM-resident filter.}
  \label{fig:query-mon02-large}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/perf/query_perf_gh200_large.png}
  \caption{Query Performance on System B (HBM3) for a DRAM-resident filter.}
  \label{fig:query-gh200-large}
\end{figure}

\FloatBarrier

\subsubsection{Deletion Performance}
Figures \ref{fig:delete-mon02-large} and \ref{fig:delete-gh200-large} illustrate the deletion performance.

The Cuckoo filter's dominance in deletion throughput is consistent across both hardware architectures. Whether bound by GDDR7 or HBM3 latency, the simplicity of the Cuckoo filter's atomic-CAS deletion logic allows it to outperform the TCF and GQF by orders of magnitude. While the GQF suffers from the latency of shifting elements to maintain sorted runs, the TCF is bottlenecked by its use of cooperative groups and block-wide updates. These heavier operations interact poorly with the high latency of global memory access compared to the single-slot atomic operation used by the Cuckoo filter.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/perf/delete_perf_mon02_large.png}
  \caption{Deletion Performance on System A (GDDR7) for a DRAM-resident filter.}
  \label{fig:delete-mon02-large}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/perf/delete_perf_gh200_large.png}
  \caption{Deletion Performance on System B (HBM3) for a DRAM-resident filter.}
  \label{fig:delete-gh200-large}
\end{figure}

\FloatBarrier

\section{Bucket Size Impact}
\label{sec:eval:bucket-size}

The \texttt{bucketSize} parameter, defining the number of fingerprints stored in each bucket, plays an important role in the filter's overall performance. Testing demonstrates that performance degrades at both the lower and upper extremes of bucket sizing, necessitating a careful balance between algorithmic overhead and memory access granularity.

\subsection{Performance Trade-offs}

\begin{itemize}
  \item \textbf{Small Buckets}: Configuring the filter with very small buckets negatively impacts performance. With fewer slots per bucket, the probability of a collision increases, as does the likelihood that a bucket is full. This increases the average number of buckets that must be accessed and loaded from memory to complete an insertion or a lookup, resulting in higher latency.

  \item \textbf{Large Buckets}: Excessively large buckets introduce hardware-level inefficiencies. If a bucket's size exceeds the GPU's cache line size (typically 128 bytes), fetching a single logical bucket requires multiple physical memory transactions. In the worst case, this doubles the number of cache lines that must be fetched, effectively halving the effective memory bandwidth.
\end{itemize}

\subsection{Optimal Configuration}

The experiments in Figure \ref{fig:bucket-size} identify distinct optimal configurations for insertion and lookup operations depending on the working set size:

\begin{itemize}
  \item \textbf{Insertion}: A bucket size of 16 fingerprints was found to be the fastest configuration in all tested scenarios. This size appears to offer the optimal trade-off, providing enough slots to minimize eviction chains without incurring the bandwidth penalty of multi-cache-line fetches.

  \item \textbf{Lookup (L2-Resident)}: When the filter is sized to fit entirely within the GPU's L2 cache, a bucket size of 8 fingerprints yields the highest throughput. This is driven by instruction-level efficiency. A bucket of this size can be represented as two 64-bit words or a single 128-bit vector. This allows the entire bucket to be loaded into registers via a single vectorized machine instruction, maximizing the throughput of the L1/L2 cache hierarchy.

  \item \textbf{Lookup (DRAM-Resident)}: Once the filter size exceeds the L2 cache capacity, the bottleneck shifts to global memory bandwidth. In this case, a bucket size of 16 becomes favourable again. Since fetching data from DRAM creates a significant latency penalty, it is more efficient to process a larger "middle ground" bucket size that maximizes the utility of the data fetched in a standard 128-byte DRAM burst.
\end{itemize}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{images/bucket-size.png}
  \caption{Normalized throughput of the GPU Cuckoo filter for different bucket sizes.}
  \label{fig:bucket-size}
\end{figure}

\FloatBarrier

\section{Eviction Policies}
\label{sec:eval:eviction-policies}

To evaluate the impact of the insertion strategy on performance and stability, a comparative analysis was conducted between the standard DFS eviction policy and the proposed BFS heuristic.

\subsection{Experimental Setup}
For this analysis and the subsequent evaluation of sorted insertion (Section \ref{sec:eval:sorted-insertion}), the experimental setup was expanded with a third hardware configuration to provide finer-grained data on GDDR7 performance scaling:

\begin{itemize}
  \item \textbf{System C (GDDR7)}: An AMD Ryzen 9 5900X (12 cores) paired with an NVIDIA RTX 5070 Ti GPU featuring 16 GB of GDDR7 memory (0.9 TB/s). The system runs NixOS 25.11 with NVIDIA driver 580.105.08 and CUDA 12.8.93.
\end{itemize}

The tests use a fixed capacity of either $2^{22}$ (L2-resident) or $2^{28}$ (DRAM-resident) slots. To accurately measure performance in the critical high-load regime, the insertion workload is split based on the target load factor $\alpha$. For each data point, the filter is first pre-filled with 75\% of the total items required to reach the target load (i.e., $0.75 \cdot \alpha \cdot capacity$). Subsequently, the remaining 25\% of items are inserted to reach the final target load $\alpha$, and the throughput of this second phase is recorded. This ensures that the measurement captures the performance behavior specifically as the filter transitions from a moderately full state to the final target occupancy, effectively isolating the impact of the eviction strategy.

\subsection{Eviction Reduction Analysis}
\label{sec:eval:eviction-policies:reduction}

The premise of the BFS eviction policy is that by investing more computational effort to search for a "local" empty slot, the filter can avoid triggering long, expensive eviction chains. To validate this hypothesis, the average number of evictions performed per inserted item was measured.

As shown in Figure \ref{fig:eviction-reduction}, the BFS policy successfully lowers the eviction rate compared to the greedy DFS approach.

As the filter fills ups, the DFS strategy (which picks a random victim immediately upon collision) sees a quick exponential increase in evictions. In contrast, the BFS strategy delays this spike significantly. By checking up to 8 candidate slots before resorting to an eviction, the BFS approach resolves many collisions locally.

This reduction in evictions directly translates to a reduction in global memory writes. Every eviction saved is an atomic read-modify-write transaction avoided. This explains the performance data observed in the next section: on bandwidth-limited systems (Systems A and C), saving these memory transactions yields a net speed-up, whereas on bandwidth-rich systems (System B), the cost of the extra local checks outweighs the savings.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{images/evict/count_per_insert.png}
  \caption{Average number of evictions per insertion on System B.}
  \label{fig:eviction-reduction}
\end{figure}

\FloatBarrier

\subsection{Performance Analysis}
The throughput impact of the BFS policy presents a trade-off between computational complexity (checking 8 candidates per step) and memory bandwidth efficiency (avoiding global memory accesses caused by evictions).

\subsubsection{L2-Resident Workloads}
In the L2-resident scenario (Figures \ref{fig:evict:insert-system-c-small}, \ref{fig:evict:insert-system-b-small}, \ref{fig:evict:insert-system-a-small}), the standard DFS policy consistently outperforms the BFS policy across all systems.

When the filter fits entirely within the L2 cache, the latency penalty of an eviction (loading a new bucket) is minimal. At the same time, the BFS policy incurs a higher instruction overhead per step because it must perform atomic checks on up to 8 candidate slots within the loaded buckets. In this bandwidth-abundant, low-latency environment, the computational cost of these extra checks outweighs the savings from reduced evictions, making the simpler greedy approach faster.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/evict/insert_system_a_small.png}
  \caption{Insert Throughput on System A (L2-Resident).}
  \label{fig:evict:insert-system-a-small}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/evict/insert_system_b_small.png}
  \caption{Insert Throughput on System B (L2-Resident).}
  \label{fig:evict:insert-system-b-small}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/evict/insert_system_c_small.png}
  \caption{Insert Throughput on System C (L2-Resident).}
  \label{fig:evict:insert-system-c-small}
\end{figure}

\FloatBarrier

\subsubsection{DRAM-Resident Workloads}

For large filters, the performance dynamics shift based on the memory technology of the underlying hardware.

\begin{itemize}
  \item \textbf{GDDR7 Systems (System A \& C)}: On both the RTX Pro 6000 and the RTX 5070 Ti, the BFS policy achieves higher throughput than the DFS policy at high load factors (Figure \ref{fig:evict:insert-system-a-large}, \ref{fig:evict:insert-system-c-large}). On these systems, the latency and bandwidth cost of fetching a new bucket from DRAM (caused by an eviction) is high. By investing more compute cycles to find a slot locally, the BFS policy effectively reduces global memory traffic, resulting in a net performance gain.

  \item \textbf{HBM3 System (System B)}: Conversely, on the GH200 (Figure \ref{fig:evict:insert-system-b-large}), the standard DFS policy remains faster. The massive bandwidth provided by HBM3 effectively hides the cost of the extra evictions generated by the DFS policy. Consequently, the bottleneck shifts back to compute latency, where the simpler logic of the DFS approach proves superior.
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/evict/insert_system_a_large.png}
  \caption{Insert Throughput on System A (DRAM-Resident).}
  \label{fig:evict:insert-system-a-large}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/evict/insert_system_b_large.png}
  \caption{Insert Throughput on System B (DRAM-Resident).}
  \label{fig:evict:insert-system-b-large}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/evict/insert_system_c_large.png}
  \caption{Insert Throughput on System C (DRAM-Resident).}
  \label{fig:evict:insert-system-c-large}
\end{figure}

\FloatBarrier

\section{Sorted vs. Unsorted Insertion}
\label{sec:eval:sorted-insertion}

To investigate the potential for enhancing insertion throughput by improving memory locality, an alternative insertion strategy was implemented and evaluated. This strategy, detailed in Section \ref{sec:implementation:optimisation-techniques:sorted}, pre-sorts the input keys by their primary bucket index before launching the insertion kernel. The goal is to maximize coalesced memory accesses by ensuring that adjacent threads target the same or nearby memory regions.

\subsection{Performance Characteristics}

The performance impact of sorting was evaluated across all three test systems. Figures \ref{fig:sorted-system-a}, \ref{fig:sorted-system-b}, and \ref{fig:sorted-system-c} illustrate the throughput comparison between the standard unsorted approach and the sorted variant across a wide range of input batch sizes.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/sorted_insertion/system_a.png}
  \caption{Throughput comparison of sorted vs. unsorted insertion on System A (GDDR7).}
  \label{fig:sorted-system-a}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/sorted_insertion/system_b.png}
  \caption{Throughput comparison of sorted vs. unsorted insertion on System B (HBM3).}
  \label{fig:sorted-system-b}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/sorted_insertion/system_c.png}
  \caption{Throughput comparison of sorted vs. unsorted insertion on System C (GDDR7).}
  \label{fig:sorted-system-c}
\end{figure}

\FloatBarrier

\subsubsection{Impact on Throughput Curve}
Across all hardware configurations, pre-sorting the input has a distinct "flattening" effect on the throughput curve.
\begin{itemize}
  \item \textbf{Small Inputs (L2-Resident)}: For smaller input sizes, where the filter fits within the L2 cache, the sorted strategy is significantly slower. The overhead of the radix sort dominates the total execution time, and the benefits of memory locality are minimal since the data is already cached.
  \item \textbf{Large Inputs (DRAM-Resident)}: As the input size grows and the filter spills into global memory, the unsorted throughput declines due to random memory access latency. Conversely, the sorted throughput remains relatively stable.
\end{itemize}

\subsubsection{Architecture-Specific Behavior}
The usefulness of this optimization is heavily dependent on the memory subsystem of the GPU.

\begin{itemize}
  \item \textbf{GDDR7 Systems (A \& C)}: On Systems A and C, which rely on GDDR7 memory, the sorted insertion strategy begins to show promise at the upper limits of the tested capacity. As the memory bottleneck intensifies, the improved access patterns of the sorted approach allow it to match or slightly exceed the performance of the unsorted baseline. This confirms that sorting serves as a valid optimization for scenarios that are severely bound by global memory latency.

  \item \textbf{HBM3 System (B)}: On System B, the massive bandwidth of HBM3 effectively hides the random access latency of the unsorted approach. Consequently, the unsorted insertion remains significantly faster across all tested sizes. The cost of sorting simply adds overhead without providing a necessary benefit, as the memory controller can already saturate the bus with random requests.
\end{itemize}

\subsection{Trade-offs and Limitations}

While sorting can theoretically improve throughput for memory-bound workloads, it introduces significant practical limitations:

\begin{itemize}
  \item \textbf{Memory Overhead}: The sorting process is not free in terms of space. It requires auxiliary buffers to hold the sorted keys and indices, effectively doubling the peak memory usage during insertion. This halves the maximum batch size that can be processed in a single pass, which may be prohibitive for memory-constrained applications.

  \item \textbf{Limited Gains}: Even on GDDR7 systems where the optimization is most relevant, the crossover point where sorting becomes beneficial occurs only at very large capacities. Given the substantial memory overhead, the modest throughput gains at the extreme tail end of the capacity curve may not justify the added complexity and resource consumption for general-use cases.
\end{itemize}

\section{False Positive Rate (FPR)}
\label{sec:eval:fpr}

To evaluate the reliability of the implemented filters, the empirical false-positive rate was measured across a range of filter capacities. For each test, the filters were populated to a constant 95\% load factor using random keys. The total memory size was varied from $2^{15}$ to $2^{30}$ bytes, allowing each implementation to optimize its internal layout within that fixed memory constraint. The results are presented in Figure \ref{fig:fpr-vs-memory}:

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/fpr.png}
  \caption{Comparison of False Positive Rates (FPR) versus total memory size for various filter implementations at a 95\% load factor.}
  \label{fig:fpr-vs-memory}
\end{figure}

\begin{itemize}
  \item \textbf{Blocked Bloom Filter}: The Blocked Bloom filter demonstrates the highest false-positive rate among all tested structures, ranging from approximately 0.5\% to over 4\%. This is a known characteristic of the blocked design: partitioning the bit array into small, fixed-size blocks prevents the "averaging" of hash collisions across the entire filter. Consequently, a few heavily congested blocks can disproportionately skew the overall error rate. It is notably the only filter where the false-positive rate actively degrades as the total memory size increases.

  \item \textbf{Quotient Filter Accuracy}: The GQF exhibits the lowest false-positive rate among all candidates, maintaining an error rate between 0.001\% and 0.002\%. This confirms the theoretical space efficiency of quotient filters, which handle collisions via Robin Hood hashing and metadata encoding rather than the probabilistic bucket overflow mechanisms found in Cuckoo filters.

  \item \textbf{CPU vs. GPU Cuckoo Filters}: A distinction remains visible between the CPU and GPU Cuckoo filter implementations. The CPU version achieves a very low false-positive rate, hovering near 0.005\%. The GPU Cuckoo filter, while still highly accurate, exhibits a slightly higher rate of approximately 0.045\%. This difference is a direct consequence of the bucket size trade-off discussed in Section \ref{sec:eval:bucket-size}. To maximize parallel throughput, the GPU implementation uses a bucket size of 16, whereas the CPU versions use a standard bucket size of 4. As established in Equation \ref{eq:cuckoo-space-bound}, a larger bucket size directly increases the collision probability for a fixed fingerprint size.

  \item \textbf{Comparison with TCF}: The GPU Cuckoo filter significantly outperforms the TCF regarding accuracy. The TCF exhibits an error rate roughly an order of magnitude higher (ranging between 0.2\% and 0.5\%). While the TCF is more accurate than the Blocked Bloom filter, the Cuckoo filter and GQF designs offer superior accuracy for this workload.
\end{itemize}

\section{Cache Efficiency}
\label{sec:eval:cache}

To understand how each filter interacts with the GPU's memory hierarchy, the L1 and L2 cache hit rates were measured by profiling the relevant kernels using NVIDIA Nsight Compute (ncu) on System B. While cache hit rate is not a direct proxy for overall throughput, it provides crucial insight into the memory access patterns and architectural bottlenecks of each implementation. The results for Insert, Query, and Delete operations are presented in Figures \ref{fig:cache-insert}, \ref{fig:cache-query}, and \ref{fig:cache-delete}, respectively.

\subsection{L1 Cache Analysis}
A consistent trend across all operations is the exceptionally high L1 hit rate (near 100\%) for the TCF and GQF. This is a direct consequence of their design: both filters rely heavily on shared memory for internal computations, such as intra-block sorting and cooperative group coordination. Since shared memory physically resides within the L1 cache on modern NVIDIA architectures, these accesses are counted as L1 hits.

The Cuckoo filter exhibits a moderate L1 hit rate (typically between 30\% and 60\%). This reflects its reliance on atomic operations on global memory addresses. While some repeated accesses may hit in L1, the random nature of hashing means that successive memory requests from a warp rarely map to the same L1 cache line, preventing the high hit rates seen in the shared-memory-heavy implementations. The Blocked Bloom filter generally shows the lowest L1 hit rate, consistent with its streaming access pattern where new cache lines are constantly pulled in.

\subsection{L2 Cache Analysis}

The L2 cache hit rates provide a clear visualization of the transition from a cache-resident workload to a DRAM-resident workload. For the Cuckoo filter and the Blocked Bloom filter, the L2 hit rate remains high (approximately 80-90\%) for smaller capacities. However, a sharp decline is observed once the filter size exceeds $2^{24}$ elements. This inflection point roughly corresponds to the physical L2 cache size of the test GPU. The steep drop-off confirms that beyond this point, every operation effectively incurs a global memory access, explaining the shift in performance scaling discussed in Section \ref{sec:eval:throughput}.

The GQF exhibits similar behavior for its Lookup operation (see Figure \ref{fig:cache-query}). Because a GQF lookup involves linearly scanning a cluster of slots in memory, it relies heavily on the L2 cache to minimize latency. Once the filter grows too large, these linear scans result in frequent cache misses, aligning its curve with that of the Cuckoo and Bloom filters.

In contrast, the TCF and GQF (specifically for Insertion and Deletion) maintain a consistently high L2 hit rate across all filter sizes. This unusual stability indicates that these algorithms interact with global memory far less frequently than the Cuckoo or Bloom filters. Instead, they perform the vast majority of their work—such as sorting items within a block or managing cooperative groups—using internal registers and shared memory. While this results in high cache statistics, it confirms the architectural analysis from the previous section: these filters are bound by the speed of the GPU's compute units and shared memory (SRAM), preventing them from utilizing the abundant DRAM bandwidth available on modern High-Bandwidth Memory systems.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/cache/insert.png}
  \caption{L1 and L2 cache hit rates for the Insert operation across varying filter capacities.}
  \label{fig:cache-insert}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/cache/query.png}
  \caption{L1 and L2 cache hit rates for the Query operation across varying filter capacities.}
  \label{fig:cache-query}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{images/cache/delete.png}
  \caption{L1 and L2 cache hit rates for the Delete operation across varying filter capacities.}
  \label{fig:cache-delete}
\end{figure}

\FloatBarrier

\section{Hardware Utilization}
\label{sec:eval:sol}

To validate the architectural hypotheses regarding memory-bound versus compute-bound behavior, the resource utilization of each filter was profiled using NVIDIA Nsight Compute just like in Section \ref{sec:eval:cache}. This analysis measures the achieved throughput as a percentage of the GPU's theoretical peak ("Speed of Light") for three critical subsystems: Compute (SM), Cache (L1/L2), and Global Memory (DRAM). The results are presented in Figures \ref{fig:sol-compute}, \ref{fig:sol-cache-l1}, \ref{fig:sol-cache-l2}, and \ref{fig:sol-dram}.

\subsection{Compute Utilization}
The SM throughput metrics, shown in Figure \ref{fig:sol-compute}, reveal distinct execution characteristics for the different algorithms. The Cuckoo and Blocked Bloom filters exhibit a characteristic "hump" profile. When the filter fits within the L2 cache (up to $2^{24}$ elements), compute utilization rises steadily as the execution is dominated by hash calculations and bitwise manipulation instructions. However, once the capacity exceeds the L2 limit, compute utilization drops sharply. This decline occurs because the SMs begin to stall while waiting for data from global memory, shifting the primary bottleneck from instruction throughput to memory latency.

In contrast, the TCF shows a steady increase in compute utilization as the filter grows, eventually reaching high levels of SM saturation (up to 80\%). This confirms that the TCF is primarily compute-bound, spending the majority of its cycles executing cooperative group logic and sorting operations within shared memory rather than waiting on external memory. The GQF stands out for consistently low compute utilization, particularly for insertion and deletion. This suggests it is bottlenecked by neither pure compute throughput nor memory bandwidth, but likely by serialization latency within threads, such as branch divergence or dependency stalls.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/sol/sm_throughput.png}
  \caption{Compute (SM) throughput as a percentage of peak performance.}
  \label{fig:sol-compute}
\end{figure}

\FloatBarrier

\subsection{Cache Throughput}
The L1 and L2 cache throughput metrics (Figures \ref{fig:sol-cache-l1} and \ref{fig:sol-cache-l2}) mirror the cache hit rate findings from Section \ref{sec:eval:cache}.

The Cuckoo and Blocked Bloom filters effectively utilize cache bandwidth up to the L2 capacity limit, after which throughput declines as requests miss to DRAM. In contrast, the TCF and GQF maintain slowly increasing cache throughput regardless of filter size, reinforcing that their working set for active operations remains largely resident in shared memory/L1.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/sol/l1_throughput.png}
  \caption{L1 throughput as a percentage of peak performance.}
  \label{fig:sol-cache-l1}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/sol/l2_throughput.png}
  \caption{L2 throughput as a percentage of peak performance.}
  \label{fig:sol-cache-l2}
\end{figure}

\FloatBarrier

\subsection{DRAM Throughput}
The DRAM throughput results, presented in Figure \ref{fig:sol-dram}, provide definitive confirmation of the scaling characteristics discussed in Section \ref{sec:eval:throughput}. For the Cuckoo and Blocked Bloom filters, DRAM utilization jumps significantly once the filter size exceeds the L2 cache limit. Notably, the Cuckoo filter's insert operation utilizes nearly 35\% of the peak DRAM bandwidth, while query operations reach over 60\%. This confirms that these algorithms are truly memory-bound for large datasets. Consequently, their performance is directly tied to the available memory bandwidth, ensuring they will continue to benefit from future hardware advancements like HBM3e and HBM4.

At the same time, the TCF and GQF show negligible DRAM utilization (near 0\%) for insertion and deletion operations, regardless of filter size. This effectively proves that these algorithms are unable to utilize the available global memory bandwidth. Their performance is strictly limited by the speed of the on-chip memory (SRAM). As a result, they are less likely to scale with future improvements in DRAM technology compared to the memory-hungry Cuckoo filter.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/sol/dram_throughput.png}
  \caption{Global Memory (DRAM) throughput as a percentage of peak performance.}
  \label{fig:sol-dram}
\end{figure}
