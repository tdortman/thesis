\section{Testing and Verification}
\label{sec:implementation:testing}

To ensure the correctness and reliability of the implementation, a comprehensive verification strategy was employed. This strategy combines fine-grained unit tests to validate individual components with end-to-end empirical tests designed to confirm the functional correctness and theoretical properties of the final software artifacts. The following sections detail the methodologies used for unit testing and the empirical validation of the single-GPU filter, the IPC wrapper, and the multi-GPU implementation.

\subsection{Unit Testing}
\label{sec:implementation:testing:unit}

The Googletest framework is utilized to implement a comprehensive suite of unit tests. These tests are designed to validate the correctness of individual components in isolation. The test suite covers common use cases and known edge cases (such as empty inputs, full filters, and operations on zero-capacity filters). This low-level validation provides a strong foundation of correctness upon which the larger system is built.

\subsection{Functional and Empirical Verification}
\label{sec:implementation:testing:functional}

Beyond unit tests, a dedicated test binary was created for each primary output artifact to empirically verify its end-to-end functionality. A standardized test flow was developed to rigorously validate the correctness of the filter implementations. This procedure was then adapted for the specific contexts of the single-GPU, IPC, and multi-GPU versions.

\subsubsection{Standard Verification Procedure}
\label{sec:implementation:testing:std-proc}

The core empirical validation follows a standardized, multi-stage procedure using random 64-bit integer keys:

\begin{enumerate}
  \item \textbf{Insertion}: Random keys from the range $[0, 2^{32}-1]$ are inserted into an empty filter until a specified target load factor is reached.

  \item \textbf{No-False-Negatives Test}: A query is performed for all the previously inserted keys. The test passes only if every key is successfully found, confirming that no false negatives have occurred.

  \item \textbf{False Positive Rate Measurement}: The empirical false positive rate is measured by querying one million distinct keys known not to be in the filter (drawn from the disjoint range $[2^{32}, 2^{64}-1]$). The observed rate of positive matches is then calculated and compared against the theoretically expected rate to ensure it falls within an acceptable margin.

  \item \textbf{Deletion Correctness and Stability Test}: Half of the initially inserted items are deleted. A subsequent query for the entire original set of keys is then conducted to verify three conditions:
    \begin{itemize}
      \item All non-deleted items must still be found.
      \item The vast majority of deleted items must no longer be found
      \item The rate of any remaining positive hits for the deleted items must be statistically consistent with the filter's theoretical false positive rate.
    \end{itemize}
\end{enumerate}

\subsubsection{Single-GPU Filter Verification}
\label{sec:implementation:testing:single-gpu}

The core single-GPU filter implementation was subjected to the standard verification procedure outlined above. The test binary for this version operates directly on device-side data buffers, providing a controlled environment to validate the correctness of the CUDA kernels on a single device.

\subsubsection{IPC Wrapper Verification}
\label{sec:implementation:testing:ipc}

The IPC wrapper is validated using a dedicated test binary designed to operate in one of two modes: as a server or as a client. The procedure is conducted by first manually launching an instance of the binary in server mode. Subsequently, a separate instance is launched in client mode, which then spawns multiple internal threads to concurrently send batches of requests to the running server. This test serves as an empirical proof-of-concept for the IPC mechanism, validating the end-to-end communication, command queuing, and zero-copy data transfer.

\subsubsection{Multi-GPU Verification}
\label{sec:implementation:testing:multi-gpu}

The multi-GPU implementation was also validated against the standard verification procedure. The test binary for this artifact allows for command-line configuration of the filter's capacity and the number of GPUs to utilize. A key difference from the single-GPU test is that all input and output data buffers reside in host memory. This setup rigorously exercises the entire data pipeline: from the initial scatter from the host, through the inter-GPU data exchange via Gossip, to the final consolidation of results back to the host. The tests are specifically designed to run on datasets that exceed the memory capacity of a single GPU, thereby validating the primary use case of this implementation.
