\section{Parallel Algorithms}
\label{sec:implementation:parallel-algorithms}

The core of this thesis is the design of parallel algorithms for the Cuckoo filter's primary operations: insertion, lookup, and deletion. These algorithms are designed to be launched as CUDA kernels, where many threads cooperate to process batches of items simultaneously.

\subsection{Insertion}
\label{sec:implementation:insertion}

The parallel insertion algorithm is designed to handle a large batch of items in parallel, with each CUDA thread being responsible for inserting a single item. The process for each thread is as follows:

\begin{enumerate}
  \item \textbf{Hashing and Key Generation}: Each item is first hashed into a 64-bit value using the xxHash64 algorithm, chosen for its high speed and excellent statistical properties. This hash is then split: the upper 32 bits are used to derive the item's fingerprint, while the lower 32 bits are used as the basis for its primary bucket index. A crucial initial finding was that using the same bits for both the fingerprint and the primary index led to a high probability of identical fingerprints clustering in the same buckets, causing a severe degradation in performance. Thus, distinct parts of the hash are used for each. The alternate bucket index is then calculated using the partial-key cuckoo hashing scheme, which is described in detail in Section \ref{sec:background:cuckoo-filter}.

  \item \textbf{Direct Insertion Attempt}: The thread first checks the two candidate buckets for an empty slot. It uses the item's fingerprint to generate a random starting offset within each bucket and performs a linear search from that point. If an empty slot is found in either bucket, the fingerprint is inserted via an atomic operation, and the insertion for that item succeeds.

  \item \textbf{Eviction Process}: If both candidate buckets are full, the thread initiates the cuckoo process. It randomly selects one of the two buckets and a random occupied slot within it. It then atomically replaces the fingerprint (\texttt{tag\_old}) in that slot with its own fingerprint (\texttt{tag\_new}). The evicted fingerprint, \texttt{tag\_old}, now becomes the item that the thread must insert. The thread calculates \texttt{tag\_old}'s alternate bucket and continues this process in a loop.

  \item \textbf{Termination}: The eviction loop continues until either an empty slot is found or a predefined limit on the number of evictions is reached, at which point the insertion is reported as a failure.

\end{enumerate}

To maintain an accurate count of the total items in the filter without creating a bottleneck on a single atomic counter, a hierarchical reduction is employed. Each thread that successfully inserts an item contributes a +1. These values are first summed efficiently at the warp level using shuffle instructions, then aggregated at the block level using shared memory, and finally, a single atomic addition per block is performed on the global counter in device memory.

% FIXME: I don't really like this formatting but that's okay, can always change it later
% tex-fmt: off
\begin{algorithm}[H]
  \caption{Parallel Insertion}
  \label{alg:parallel-insertion}
\begin{algorithmic}[0]
  \Function{Insert}{key}
    \State $h = \mathrm{hash}(\text{key})$
    \State $fp = \mathrm{upper32}(h) \mathbin{\&} \bigl((1 \ll \text{bitsPerTag}) - 1\bigr)$
    \State $i_1 = \mathrm{lower32}(h) \bmod \text{numBuckets}$
    \State $i_2 = \text{alt\_bucket}(i_1, fp)$
    \If{there is an empty slot in bucket $i_1$ or $i_2$}
      \State insert $fp$ into an empty slot (atomic)
      \State \textbf{return} \texttt{Success}
    \Else
      \State $\text{tag} = fp$
      \State $b =$ randomly pick $i_1$ or $i_2$
      \For{$n = 1$ \textbf{to} \text{maxEvictions}}
        \State $s =$ random slot in bucket $b$
        \State $\text{tag} = \text{atomic\_exchange}(buckets[b][s],\ \text{tag})$
        \State $b = \text{alt\_bucket}(b, \text{tag})$
        \If{there is an empty slot in bucket $b$}
          \State insert \text{tag} into an empty slot (atomic)
          \State \textbf{return} \texttt{Success}
        \EndIf
      \EndFor
      \LComment{Filter is considered full}
      \LComment{Caller will have to increase its capacity}
      \State \textbf{return} \texttt{Failure}
    \EndIf
  \EndFunction
\end{algorithmic}
\end{algorithm}
% tex-fmt: on

\subsection{Lookup}
\label{sec:implementation:lookup}

The parallel lookup algorithm is similar to insertion but is a read-only operation, allowing for more aggressive memory access optimizations. Each thread is assigned an item to look for.

The thread calculates the item's fingerprint and its two candidate bucket indices, just as in the insertion process. It then searches both buckets for a matching fingerprint, again using the fingerprint to determine a random starting point for a linear probe. Despite the ability to use two threads per item to check both buckets in parallel, this was found to be slower in every tested case.

The key optimization in the lookup kernel is the use of vectorized, non-atomic memory loads. Since it can be guaranteed that no other kernel is writing or deleting during a lookup operation, thread safety is not a concern. Modern GPU hardware can load 128 bits (or 16 bytes) in a single instruction. The kernel leverages this by loading two 64-bit words at a time, allowing a thread to check multiple fingerprints with a single memory transaction and dramatically increasing throughput. The results of the lookup (found or not found) are written to a user-provided output array.

% tex-fmt: off
\begin{algorithm}[H]
  \caption{Parallel Lookup}
  \label{alg:parallel-lookup}
\begin{algorithmic}[0]
  \Function{Contains}{key}
    \State $h = \mathrm{hash}(\text{key})$
    \State $fp = \mathrm{upper32}(h) \mathbin{\&} \bigl((1 \ll \text{bitsPerTag}) - 1\bigr)$
    \State $i_1 = \mathrm{lower32}(h) \bmod \text{numBuckets}$
    \State $i_2 = i_1 \oplus \mathrm{hash}(fp) \bmod \text{numBuckets}$
    \State \textbf{return} FindSlot(bucket $i_1$, $fp$) \textbf{or} FindSlot(bucket $i_2$, $fp$)
  \EndFunction
  \Statex
  \Function{FindSlot}{bucket, tag}
    \State startSlot $=$ tag $\bmod$ bucketSize
    \State startWord $=$ startSlot / tagsPerWord
    \State startPair $=$ floorToEven(startWord)

    \For{$i = 0$ \textbf{to} wordCount / 2}
        \State pairIdx $=$ (startPair + i * 2) $\bmod$ wordCount
        \State word1, word2 $=$ loadTwoWordsAt(bucket, pairIdx)

        \For{each word \textbf{in} \{word1, word2\}}
            \For{$j = 0$ \textbf{to} tagsPerWord}
                \If{extractTag(word, $j$) == tag}
                    \State \textbf{return} \texttt{Success}
                \EndIf
            \EndFor
        \EndFor
    \EndFor
    \State \textbf{return} \texttt{Failure}
  \EndFunction
\end{algorithmic}
\end{algorithm}
% tex-fmt: on
\subsection{Deletion}
\label{sec:implementation:deletion}

The parallel deletion algorithm is designed to be robust against race conditions, especially in the rare but possible case of multiple identical fingerprints occupying the same candidate buckets. Each thread assigned a deletion task searches through its two candidate buckets for the target fingerprint. Upon finding a potential match, the thread performs an atomic compare-and-swap (CAS) operation to replace the fingerprint with an "empty" marker (zero).

The outcome of this atomic operation dictates the thread's next action:

\begin{itemize}
  \item \textbf{If the CAS succeeds}, the thread has successfully located and removed its target. The deletion is considered complete, and the thread terminates its search.
  \item \textbf{If the CAS fails}, it implies that another thread has modified that specific slot in the brief moment after it was found (e.g., by deleting a different item that happened to have the same fingerprint). In this event, the thread's deletion is not yet complete, and it must continue its linear scan through the rest of the bucket(s) to find another instance of the fingerprint to attempt to remove.
\end{itemize}

Each deletion attempt proceeds until an entry has been successfully removed, or both buckets have been fully examined. This approach ensures the operation is handled independently and correctly, even when duplicate fingerprints exist. The success or failure of each operation can be optionally reported back to the host via a provided bit-vector. Similarly to how it is done after insertion, a hierarchical reduction on the success values is used to minimize contention when decrementing the atomic counter that holds the number of occupied slots in the filter.

% tex-fmt: off
\begin{algorithm}[H]
  \caption{Parallel Deletion}
  \label{alg:parallel-deletion}
\begin{algorithmic}[0]
  \Function{Remove}{key}
    \State $h = \mathrm{hash}(\text{key})$
    \State $fp = \mathrm{upper32}(h) \mathbin{\&} \bigl((1 \ll \text{bitsPerTag}) - 1\bigr)$
    \State $i_1 = \mathrm{lower32}(h) \bmod \text{numBuckets}$
    \State $i_2 = \text{alt\_bucket}(i_1, fp)$
    \State \textbf{return} TryRemove(bucket $i_1$, $fp$) \textbf{or} TryRemove(bucket $i_2$, $fp$)
  \EndFunction
  \Statex
  \Function{TryRemove}{bucket, tag}
    \State startWord $= (\text{tag} \bmod \text{bucketSize}) / \text{tagsPerWord}$
    \For{$i = 0$ \textbf{to} wordCount - 1}
        \State currWord $= (\text{startWord} + i) \bmod \text{wordCount}$
        \For{$j = 0$ \textbf{to} tagsPerWord - 1}
            \If{currWord[$j$] $==$ tag}
                \If{atomic\_exchange(currWord[$j$], \text{EMPTY}) $==$ tag}
                    \State \textbf{return} \texttt{Success}
                \EndIf
            \EndIf
        \EndFor
    \EndFor
    \State \textbf{return} \texttt{Failure}
  \EndFunction
\end{algorithmic}
\end{algorithm}
% tex-fmt: on
